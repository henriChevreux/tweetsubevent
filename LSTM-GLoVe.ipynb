{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-12 09:25:15.723994: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import gensim.downloader as api\n",
    "import re\n",
    "import swifter\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import GlorotUniform, Orthogonal\n",
    "import random\n",
    "# Enable tqdm for pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# Ensure Reproducibility\n",
    "import random\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "\n",
    "# Python's built-in random\n",
    "random.seed(SEED)\n",
    "\n",
    "# NumPy\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# TensorFlow\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Set Python hash seed\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "\n",
    "# Configure TensorFlow for deterministic operations\n",
    "tf.keras.utils.set_random_seed(SEED)  # Sets all random seeds for the program (Python, NumPy, and TensorFlow)\n",
    "tf.config.experimental.enable_op_determinism()  # Enable deterministic operations in TensorFlow\n",
    "\n",
    "# If using GPU, you might also want to set these:\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    # Force TensorFlow to use deterministic GPU operations\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "    # Limit GPU memory growth\n",
    "    for gpu in tf.config.experimental.list_physical_devices('GPU'):\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # Limit to one GPU if using multiple GPUs\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe model\n",
    "glove_model = api.load(\"glove-twitter-200\")  # 200-dimensional GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Country list for team extraction\n",
    "country_list = [\n",
    "    \"Argentina\", \"Belgium\", \"Germany\", \"Serbia\", \"Greece\", \"IvoryCoast\", \n",
    "    \"Netherlands\", \"Mexico\", \"Australia\", \"Spain\", \"SouthKorea\", \n",
    "    \"Cameroon\", \"Brazil\", \"France\", \"Nigeria\", \"Algeria\", \"USA\", \n",
    "    \"Honduras\", \"Switzerland\", \"Croatia\", \"Chile\", \"Portugal\", \n",
    "    \"Ghana\", \"Slovenia\"\n",
    "]\n",
    "\n",
    "country_variations = {\n",
    "    'argentina': ['argentina', 'arg', 'argentine', 'argies', 'albiceleste', 'argentinian', 'argentinos', 'argentinas'],\n",
    "    'australia': ['australia', 'aus', 'aussie', 'aussies', 'socceroos', 'oz', 'straya', 'australian', 'au'],\n",
    "    'belgium': ['belgium', 'bel', 'belgique', 'belgie', 'belgian', 'belgians', 'red devils', 'diables rouges'],\n",
    "    'brazil': ['brazil', 'bra', 'brasil', 'bresil', 'brazilian', 'brazilians', 'selecao', 'canarinho', 'verde amarela', 'samba boys'],\n",
    "    'cameroon': ['cameroon', 'cmr', 'cameroun', 'camerounais', 'indomitable lions', 'lions'],\n",
    "    'france': ['france', 'fra', 'french', 'les bleus', 'tricolore', 'tricolores', 'equipe de france', 'allez les bleus'],\n",
    "    'honduras': ['honduras', 'hon', 'honduran', 'hondurans', 'los catrachos', 'catrachos', 'la h'],\n",
    "    'portugal': ['portugal', 'por', 'portuguese', 'selecao das quinas', 'seleccao', 'navegadores', 'team portugal'],\n",
    "    'spain': ['spain', 'esp', 'espana', 'espania', 'spanish', 'la roja', 'furia roja', 'la furia', 'la seleccion'],\n",
    "    'southkorea': ['south korea', 'korea', 'kor', 'skorea', 'korean', 'koreans', 'taeguk warriors', 'warriors'],\n",
    "    'switzerland': ['switzerland', 'sui', 'suisse', 'schweiz', 'swiss', 'nati', 'rossocrociati', 'a team'],\n",
    "    'usa': ['usa', 'united states', 'america', 'united states of america', 'us', 'usa', 'usmnt', 'americans', 'american', 'yanks', 'uncle sam', 'stars and stripes', 'team usa'],\n",
    "    'ghana': ['ghana', 'gha', 'ghanaian', 'ghanaians', 'black stars', 'stars'],\n",
    "    'netherlands': ['netherlands', 'ned', 'holland', 'dutch', 'oranje', 'flying dutchmen', 'orange', 'clockwork orange', 'nederlands'],\n",
    "    'germany': ['germany', 'ger', 'alemania', 'deutschland', 'german', 'germans', 'die mannschaft', 'nationalelf', 'deu'],\n",
    "    'iran': ['iran', 'irn', 'iranian', 'iranians', 'team melli', 'persian stars'],\n",
    "    'nigeria': ['nigeria', 'nga', 'naija', 'super eagles', 'eagles', 'nigerian', 'nigerians', 'green eagles'],\n",
    "    'algeria': ['algeria', 'alg', 'algerian', 'algerians', 'fennecs', 'desert foxes', 'les verts'],\n",
    "    'croatia': ['croatia', 'cro', 'hrvatska', 'hrv', 'croatian', 'croatians', 'vatreni', 'blazers', 'kockasti'],\n",
    "    'chile': ['chile', 'chi', 'chilean', 'chileans', 'la roja', 'team chile'],\n",
    "    'slovenia': ['slovenia', 'svn', 'slovenian', 'slovenians', 'slovenski', 'boys'],\n",
    "    'serbia': ['serbia', 'srb', 'serbian', 'serbians', 'beli orlovi', 'white eagles', 'orlovi'],\n",
    "    'greece': ['greece', 'gre', 'greek', 'greeks', 'piratiko', 'ethniki', 'galanolefki'],\n",
    "    'ivorycoast': ['ivory coast', 'civ', 'cote divoire', 'cotedivoire', 'ivorians', 'les elephants', 'elephants', 'ivory'],\n",
    "    'mexico': ['mexico', 'mex', 'mexiko', 'mexican', 'mexicans', 'el tri', 'tricolor', 'aztecas', 'el tricolor', 'verde']\n",
    "}\n",
    "\n",
    "# Define the country code mapping\n",
    "country_code_mapping = {\n",
    "    \"Argentina\": \"ar\", \"Belgium\": \"be\", \"Germany\": \"de\", \"Serbia\": \"rs\", \"Greece\": \"gr\",\n",
    "    \"IvoryCoast\": \"ci\", \"Netherlands\": \"nl\", \"Mexico\": \"mx\", \"Australia\": \"au\", \"Spain\": \"es\",\n",
    "    \"SouthKorea\": \"kr\", \"Cameroon\": \"cm\", \"Brazil\": \"br\", \"France\": \"fr\", \"Nigeria\": \"ng\",\n",
    "    \"Algeria\": \"dz\", \"USA\": \"us\", \"Honduras\": \"hn\", \"Switzerland\": \"ch\", \"Croatia\": \"hr\",\n",
    "    \"Chile\": \"cl\", \"Portugal\": \"pt\", \"Ghana\": \"gh\", \"Slovenia\": \"si\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the average word vector for a tweet\n",
    "def get_avg_embedding(tweet, model, vector_size=200):\n",
    "    words = tweet.split()  # Tokenize by whitespace\n",
    "    word_vectors = [model[word] for word in words if word in model]\n",
    "    if not word_vectors:  # If no words in the tweet are in the vocabulary, return a zero vector\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "def extract_teams_from_filename(filename):\n",
    "    # Remove numbers and file extension\n",
    "    base_name = re.sub(r'\\d+\\.csv$', '', filename)\n",
    "    \n",
    "    # Identify teams from the predefined country list\n",
    "    teams = [country for country in country_list if country in base_name]\n",
    "    \n",
    "    if len(teams) >= 2:\n",
    "        return teams[0], teams[1]\n",
    "    elif len(teams) == 1:\n",
    "        return teams[0], \"Unknown\"\n",
    "    else:\n",
    "        return \"Unknown\", \"Unknown\"\n",
    "    \n",
    "def normalize_countries_with_teams(tweet, home_team, away_team):\n",
    "    \"\"\"\n",
    "    Normalize country mentions in a tweet by replacing them with 'hometeam', 'awayteam', or 'othercountry'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure tweet is split into words\n",
    "    if isinstance(tweet, str):\n",
    "        words = tweet.split()\n",
    "    else:\n",
    "        raise ValueError(\"Tweet is not a string.\")\n",
    "    \n",
    "    # Create a copy of words for modification\n",
    "    normalized_words = words[:]\n",
    "    \n",
    "    # Replace country mentions\n",
    "    for i, word in enumerate(words):\n",
    "        if word in country_variations.get(home_team.lower(), []):\n",
    "            normalized_words[i] = 'hometeam'\n",
    "        elif word in country_variations.get(away_team.lower(), []):\n",
    "            normalized_words[i] = 'awayteam'\n",
    "        else:\n",
    "            for country, variations in country_variations.items():\n",
    "                if word in variations and country.lower() not in [home_team.lower(), away_team.lower()]:\n",
    "                    normalized_words[i] = 'othercountry'\n",
    "                    break\n",
    "    \n",
    "    normalized_tweet = \" \".join(normalized_words)\n",
    "    return normalized_tweet\n",
    "\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Tokenization\n",
    "    words = text.split()\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Function to map team names to country codes\n",
    "def map_team_to_country_code(team_name, country_code_mapping):\n",
    "    return country_code_mapping.get(team_name, \"unknown\")\n",
    "\n",
    "\n",
    "def remove_player_names(row, football_player_data):\n",
    "    \"\"\"\n",
    "    Replace player names in the tweet with 'footballplayername'.\n",
    "    \"\"\"    \n",
    "    # Tokenize the tweet\n",
    "    words = row['Tweet'].split()\n",
    "    \n",
    "    # Normalize player names\n",
    "    normalized_words = [\n",
    "        'footballplayername' if word in football_player_data else word for word in words\n",
    "    ]\n",
    "    \n",
    "    return ' '.join(normalized_words)\n",
    "\n",
    "# Function to extract mentions, excluding those from retweets\n",
    "def extract_mentions(text):\n",
    "    # Find all mentions in the text\n",
    "    mentions = re.findall(r'@\\w+', text)\n",
    "    # Exclude mentions in the format \"RT @NAME:\"\n",
    "    filtered_mentions = [\n",
    "        mention for mention in mentions if not re.search(rf'^RT {mention}:', text)\n",
    "    ]\n",
    "    return filtered_mentions\n",
    "\n",
    "# Function to extract mentions, excluding those from retweets\n",
    "def extract_mentions(text):\n",
    "    # Find all mentions in the text\n",
    "    hashtags = re.findall(r'#\\w+', text)\n",
    "\n",
    "    return hashtags\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 86843/86843 [00:00<00:00, 332067.29it/s]\n",
      "Pandas Apply: 100%|██████████| 86843/86843 [00:00<00:00, 180427.85it/s]\n",
      "Pandas Apply: 100%|██████████| 86843/86843 [00:29<00:00, 2918.44it/s]\n",
      "Pandas Apply: 100%|██████████| 272389/272389 [00:01<00:00, 220595.56it/s]\n",
      "Pandas Apply: 100%|██████████| 272389/272389 [00:00<00:00, 283355.14it/s]\n",
      "Pandas Apply: 100%|██████████| 272389/272389 [01:19<00:00, 3438.58it/s]\n",
      "Pandas Apply: 100%|██████████| 148298/148298 [00:00<00:00, 329383.54it/s]\n",
      "Pandas Apply: 100%|██████████| 148298/148298 [00:00<00:00, 313498.74it/s]\n",
      "Pandas Apply: 100%|██████████| 148298/148298 [00:48<00:00, 3048.99it/s]\n",
      "Pandas Apply: 100%|██████████| 973985/973985 [00:03<00:00, 313143.99it/s]\n",
      "Pandas Apply: 100%|██████████| 973985/973985 [00:02<00:00, 344643.02it/s]\n",
      "Pandas Apply: 100%|██████████| 973985/973985 [04:40<00:00, 3471.39it/s]\n",
      "Pandas Apply: 100%|██████████| 99192/99192 [00:00<00:00, 354969.59it/s]\n",
      "Pandas Apply: 100%|██████████| 99192/99192 [00:00<00:00, 110504.98it/s]\n",
      "Pandas Apply: 100%|██████████| 99192/99192 [00:31<00:00, 3125.05it/s]\n",
      "Pandas Apply: 100%|██████████| 95108/95108 [00:00<00:00, 366514.54it/s]\n",
      "Pandas Apply: 100%|██████████| 95108/95108 [00:00<00:00, 369702.76it/s]\n",
      "Pandas Apply: 100%|██████████| 95108/95108 [00:31<00:00, 3062.75it/s]\n",
      "Pandas Apply: 100%|██████████| 712525/712525 [00:02<00:00, 345684.73it/s]\n",
      "Pandas Apply: 100%|██████████| 712525/712525 [00:02<00:00, 313760.63it/s]\n",
      "Pandas Apply: 100%|██████████| 712525/712525 [03:29<00:00, 3395.56it/s]\n",
      "Pandas Apply: 100%|██████████| 525725/525725 [00:02<00:00, 222198.40it/s]\n",
      "Pandas Apply: 100%|██████████| 525725/525725 [00:01<00:00, 270848.36it/s]\n",
      "Pandas Apply: 100%|██████████| 525725/525725 [02:40<00:00, 3282.89it/s]\n",
      "Pandas Apply: 100%|██████████| 155549/155549 [00:00<00:00, 347778.97it/s]\n",
      "Pandas Apply: 100%|██████████| 155549/155549 [00:00<00:00, 329469.17it/s]\n",
      "Pandas Apply: 100%|██████████| 155549/155549 [00:46<00:00, 3323.27it/s]\n",
      "Pandas Apply: 100%|██████████| 367899/367899 [00:01<00:00, 250664.64it/s]\n",
      "Pandas Apply: 100%|██████████| 367899/367899 [00:01<00:00, 299045.05it/s]\n",
      "Pandas Apply: 100%|██████████| 367899/367899 [01:52<00:00, 3280.01it/s]\n",
      "Pandas Apply: 100%|██████████| 96834/96834 [00:00<00:00, 335853.70it/s]\n",
      "Pandas Apply: 100%|██████████| 96834/96834 [00:00<00:00, 123569.66it/s]\n",
      "Pandas Apply: 100%|██████████| 96834/96834 [00:29<00:00, 3230.90it/s]\n",
      "Pandas Apply: 100%|██████████| 41539/41539 [00:00<00:00, 309934.49it/s]\n",
      "Pandas Apply: 100%|██████████| 41539/41539 [00:00<00:00, 338773.59it/s]\n",
      "Pandas Apply: 100%|██████████| 41539/41539 [00:12<00:00, 3236.59it/s]\n",
      "Pandas Apply: 100%|██████████| 824241/824241 [00:02<00:00, 304468.28it/s]\n",
      "Pandas Apply: 100%|██████████| 824241/824241 [00:02<00:00, 296902.38it/s]\n",
      "Pandas Apply: 100%|██████████| 824241/824241 [04:03<00:00, 3389.26it/s]\n",
      "Pandas Apply: 100%|██████████| 313803/313803 [00:01<00:00, 236204.60it/s]\n",
      "Pandas Apply: 100%|██████████| 313803/313803 [00:00<00:00, 422114.80it/s]\n",
      "Pandas Apply: 100%|██████████| 313803/313803 [01:35<00:00, 3279.61it/s]\n",
      "Pandas Apply: 100%|██████████| 85675/85675 [00:00<00:00, 407442.30it/s]\n",
      "Pandas Apply: 100%|██████████| 85675/85675 [00:00<00:00, 408291.33it/s]\n",
      "Pandas Apply: 100%|██████████| 85675/85675 [00:26<00:00, 3240.41it/s]\n",
      "Pandas Apply: 100%|██████████| 256445/256445 [00:01<00:00, 197345.67it/s]\n",
      "Pandas Apply: 100%|██████████| 256445/256445 [00:00<00:00, 282468.25it/s]\n",
      "Pandas Apply: 100%|██████████| 256445/256445 [01:16<00:00, 3360.44it/s]\n"
     ]
    }
   ],
   "source": [
    "#####PREPROCESSING TRAIN DATA AND SAVING IT#####\n",
    "\n",
    "# # Directories\n",
    "input_folder = \"train_tweets\"\n",
    "output_folder = \"train_tweets_original_preprocess\"\n",
    "os.makedirs(output_folder, exist_ok=True)  # Ensure the output folder exists\n",
    "\n",
    "# Process each file in the input folder\n",
    "csv_files = [os.path.join(input_folder, f) for f in os.listdir(input_folder) if f.endswith(\".csv\")]\n",
    "for file_path in csv_files:\n",
    "    # Load the CSV\n",
    "    current_df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Extract teams from filename\n",
    "    filename = os.path.basename(file_path)\n",
    "    home_team, away_team = extract_teams_from_filename(filename) \n",
    "    current_df['HomeTeam'] = home_team\n",
    "    current_df['AwayTeam'] = away_team\n",
    "    \n",
    "# Extract mentions from each tweet using swifter\n",
    "    current_df['Mentions'] = current_df['Tweet'].swifter.apply(extract_mentions)\n",
    "    current_df['hashtags'] = current_df['Tweet'].swifter.apply(extract_mentions)\n",
    "    current_df['is_RT'] = current_df['Tweet'].str.startswith('RT')\n",
    "    \n",
    "    # Preprocess the tweets\n",
    "    current_df['Tweet'] = current_df.swifter.apply(lambda row: preprocess_text(row['Tweet']), axis=1)\n",
    "    \n",
    "    # Save the preprocessed data to the output folder\n",
    "    output_path = os.path.join(output_folder, filename)\n",
    "    current_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 86843/86843 [00:00<00:00, 408622.79it/s]\n",
      "Pandas Apply: 100%|██████████| 272389/272389 [00:00<00:00, 494769.77it/s]\n",
      "Pandas Apply: 100%|██████████| 148298/148298 [00:00<00:00, 512110.11it/s]\n",
      "Pandas Apply: 100%|██████████| 973985/973985 [00:01<00:00, 508756.90it/s]\n",
      "Pandas Apply: 100%|██████████| 99192/99192 [00:00<00:00, 435926.17it/s]\n",
      "Pandas Apply: 100%|██████████| 95108/95108 [00:00<00:00, 490404.11it/s]\n",
      "Pandas Apply: 100%|██████████| 712525/712525 [00:01<00:00, 537395.46it/s]\n",
      "Pandas Apply: 100%|██████████| 525725/525725 [00:00<00:00, 533097.07it/s]\n",
      "Pandas Apply: 100%|██████████| 155549/155549 [00:00<00:00, 535345.60it/s]\n",
      "Pandas Apply: 100%|██████████| 367899/367899 [00:00<00:00, 507257.30it/s]\n",
      "Pandas Apply: 100%|██████████| 96834/96834 [00:00<00:00, 412502.12it/s]\n",
      "Pandas Apply: 100%|██████████| 41539/41539 [00:00<00:00, 486359.75it/s]\n",
      "Pandas Apply: 100%|██████████| 824241/824241 [00:01<00:00, 487160.55it/s]\n",
      "Pandas Apply: 100%|██████████| 313803/313803 [00:00<00:00, 527927.98it/s]\n",
      "Pandas Apply: 100%|██████████| 85675/85675 [00:00<00:00, 482444.69it/s]\n",
      "Pandas Apply: 100%|██████████| 162243/162243 [00:00<00:00, 469979.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# # Directories\n",
    "input_folder = \"train_tweets_original_preprocess\"\n",
    "output_folder = \"train_tweets_original_preprocess_with_features\"\n",
    "os.makedirs(output_folder, exist_ok=True)  # Ensure the output folder exists\n",
    "\n",
    "# Process each file in the input folder\n",
    "csv_files = [os.path.join(input_folder, f) for f in os.listdir(input_folder) if f.endswith(\".csv\")]\n",
    "for file_path in csv_files:\n",
    "    # Load the CSV\n",
    "    current_df = pd.read_csv(file_path)\n",
    "    filename = os.path.basename(file_path)\n",
    "    \n",
    "    current_df['TweetLength'] = current_df['Tweet'].apply(len)\n",
    "    current_df['TweetCount'] = current_df.groupby(['MatchID', 'PeriodID', 'Timestamp'])['Timestamp'].transform('count')\n",
    "    current_df['WordCount'] = current_df['Tweet'].swifter.apply(lambda x: len(x.split()))\n",
    "    \n",
    "    output_path = os.path.join(output_folder, filename)\n",
    "    current_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 86843/86843 [00:08<00:00, 10564.03it/s]\n",
      "Pandas Apply: 100%|██████████| 272389/272389 [00:20<00:00, 13486.44it/s]\n",
      "Pandas Apply: 100%|██████████| 148298/148298 [00:12<00:00, 12002.11it/s]\n",
      "Pandas Apply: 100%|██████████| 973985/973985 [01:01<00:00, 15733.80it/s]\n",
      "Pandas Apply: 100%|██████████| 99192/99192 [00:07<00:00, 12642.01it/s]\n",
      "Pandas Apply: 100%|██████████| 95108/95108 [00:07<00:00, 12208.99it/s]\n",
      "Pandas Apply: 100%|██████████| 712525/712525 [00:45<00:00, 15629.46it/s]\n",
      "Pandas Apply: 100%|██████████| 525725/525725 [00:35<00:00, 14676.78it/s]\n",
      "Pandas Apply: 100%|██████████| 155549/155549 [00:12<00:00, 12200.16it/s]\n",
      "Pandas Apply: 100%|██████████| 367899/367899 [00:24<00:00, 14912.78it/s]\n",
      "Pandas Apply: 100%|██████████| 96834/96834 [00:07<00:00, 12707.94it/s]\n",
      "Pandas Apply: 100%|██████████| 41539/41539 [00:03<00:00, 12811.16it/s]\n",
      "Pandas Apply: 100%|██████████| 824241/824241 [00:51<00:00, 16037.26it/s]\n",
      "Pandas Apply: 100%|██████████| 313803/313803 [00:20<00:00, 15334.65it/s]\n",
      "Pandas Apply: 100%|██████████| 85675/85675 [00:05<00:00, 14576.26it/s]\n",
      "Pandas Apply: 100%|██████████| 162243/162243 [00:09<00:00, 16566.31it/s]\n"
     ]
    }
   ],
   "source": [
    "##### PREPROCESSING TRAIN DATA AND SAVING IT #####\n",
    "\n",
    "# Directories\n",
    "input_folder = \"train_tweets_original_preprocess_with_features\"\n",
    "output_folder = \"train_tweets_no_country\"\n",
    "os.makedirs(output_folder, exist_ok=True)  # Ensure the output folder exists\n",
    "\n",
    "# Process each file in the input folder\n",
    "csv_files = [os.path.join(input_folder, f) for f in os.listdir(input_folder) if f.endswith(\".csv\")]\n",
    "for file_path in csv_files:\n",
    "    # Load the CSV\n",
    "    current_df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Preprocess the 'Tweet' column\n",
    "    current_df['Tweet'] = current_df.swifter.apply(\n",
    "        lambda row: normalize_countries_with_teams(row['Tweet'], row['HomeTeam'], row['AwayTeam']), axis=1\n",
    "    )\n",
    "\n",
    "    # Save the processed DataFrame to the output folder\n",
    "    output_path = os.path.join(output_folder, os.path.basename(file_path))\n",
    "    current_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: train_tweets_no_country/AustraliaSpain34.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 86843/86843 [00:01<00:00, 79643.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: train_tweets_preprocessed_no_country_no_player/AustraliaSpain34.csv\n",
      "Processing: train_tweets_no_country/PortugalGhana58.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 272389/272389 [00:02<00:00, 92508.33it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: train_tweets_preprocessed_no_country_no_player/PortugalGhana58.csv\n",
      "Processing: train_tweets_no_country/CameroonBrazil36.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 148298/148298 [00:01<00:00, 86357.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: train_tweets_preprocessed_no_country_no_player/CameroonBrazil36.csv\n",
      "Processing: train_tweets_no_country/GermanyBrazil74.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 973985/973985 [00:10<00:00, 88751.24it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: train_tweets_preprocessed_no_country_no_player/GermanyBrazil74.csv\n",
      "Processing: train_tweets_no_country/BelgiumSouthKorea59.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 99192/99192 [00:01<00:00, 83555.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: train_tweets_preprocessed_no_country_no_player/BelgiumSouthKorea59.csv\n",
      "Processing: train_tweets_no_country/NetherlandsChile35.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 95108/95108 [00:01<00:00, 89069.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: train_tweets_preprocessed_no_country_no_player/NetherlandsChile35.csv\n",
      "Processing: train_tweets_no_country/GermanyAlgeria67.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 712525/712525 [00:08<00:00, 88024.84it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: train_tweets_preprocessed_no_country_no_player/GermanyAlgeria67.csv\n",
      "Processing: train_tweets_no_country/FranceGermany70.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 525725/525725 [00:06<00:00, 85860.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: train_tweets_preprocessed_no_country_no_player/FranceGermany70.csv\n",
      "Processing: train_tweets_no_country/MexicoCroatia37.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 155549/155549 [00:01<00:00, 90315.22it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: train_tweets_preprocessed_no_country_no_player/MexicoCroatia37.csv\n",
      "Processing: train_tweets_no_country/FranceNigeria66.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 367899/367899 [00:04<00:00, 86827.49it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: train_tweets_preprocessed_no_country_no_player/FranceNigeria66.csv\n",
      "Processing: train_tweets_no_country/AustraliaNetherlands29.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 96834/96834 [00:01<00:00, 90937.09it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: train_tweets_preprocessed_no_country_no_player/AustraliaNetherlands29.csv\n",
      "Processing: train_tweets_no_country/HondurasSwitzerland54.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 41539/41539 [00:00<00:00, 86359.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: train_tweets_preprocessed_no_country_no_player/HondurasSwitzerland54.csv\n",
      "Processing: train_tweets_no_country/ArgentinaGermanyFinal77.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 824241/824241 [00:09<00:00, 90504.73it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: train_tweets_preprocessed_no_country_no_player/ArgentinaGermanyFinal77.csv\n",
      "Processing: train_tweets_no_country/ArgentinaBelgium72.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 313803/313803 [00:03<00:00, 92224.15it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: train_tweets_preprocessed_no_country_no_player/ArgentinaBelgium72.csv\n",
      "Processing: train_tweets_no_country/USASlovenia2010.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 85675/85675 [00:01<00:00, 85174.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: train_tweets_preprocessed_no_country_no_player/USASlovenia2010.csv\n",
      "Processing: train_tweets_no_country/GermanyUSA57.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 162243/162243 [00:01<00:00, 93378.72it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: train_tweets_preprocessed_no_country_no_player/GermanyUSA57.csv\n"
     ]
    }
   ],
   "source": [
    "##### PREPROCESSING TRAIN DATA AND SAVING IT #####\n",
    "# Load mentions with preprocessed mentions\n",
    "mentions_with_is_name = pd.read_pickle(\"mentions_processed.pkl\")\n",
    "combined_player_names = set(mentions_with_is_name[\"PreprocessedMention\"].dropna().unique())  # Use set for faster lookup\n",
    "\n",
    "# Directories\n",
    "input_folder = \"train_tweets_no_country\"\n",
    "output_folder = \"train_tweets_preprocessed_no_country_no_player\"\n",
    "os.makedirs(output_folder, exist_ok=True)  # Ensure the output folder exists\n",
    "\n",
    "\n",
    "# Process each file in the input folder\n",
    "csv_files = [os.path.join(input_folder, f) for f in os.listdir(input_folder) if f.endswith(\".csv\")]\n",
    "for file_path in csv_files:\n",
    "    print(f\"Processing: {file_path}\")\n",
    "    current_df = pd.read_csv(file_path)\n",
    "\n",
    "    # Apply the remove_player_names function\n",
    "    current_df['Tweet'] = current_df.swifter.apply(\n",
    "        lambda row: remove_player_names(row, combined_player_names), axis=1\n",
    "    )\n",
    "    \n",
    "    # Save the preprocessed data to the output folder\n",
    "    output_path = os.path.join(output_folder, os.path.basename(file_path))\n",
    "    current_df.to_csv(output_path, index=False)\n",
    "    print(f\"Saved: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the preprocessed CSV files\n",
    "input_folder = \"train_tweets_original_preprocess_with_features\"\n",
    "\n",
    "# List all CSV files in the input folder\n",
    "csv_files = [os.path.join(input_folder, f) for f in os.listdir(input_folder) if f.endswith(\".csv\")]\n",
    "\n",
    "# Load each CSV file and concatenate them into one DataFrame\n",
    "dataframes = [pd.read_csv(file) for file in csv_files]\n",
    "combined_trained_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "combined_trained_df.to_pickle('preprocessed_train_original.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the preprocessed CSV files\n",
    "input_folder = \"train_tweets_no_country\"\n",
    "\n",
    "# List all CSV files in the input folder\n",
    "csv_files = [os.path.join(input_folder, f) for f in os.listdir(input_folder) if f.endswith(\".csv\")]\n",
    "\n",
    "# Load each CSV file and concatenate them into one DataFrame\n",
    "dataframes = [pd.read_csv(file) for file in csv_files]\n",
    "combined_trained_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "combined_trained_df.to_pickle('preprocessed_train_no_country.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the preprocessed CSV files\n",
    "input_folder = \"train_tweets_preprocessed_no_country_no_player\"\n",
    "\n",
    "# List all CSV files in the input folder\n",
    "csv_files = [os.path.join(input_folder, f) for f in os.listdir(input_folder) if f.endswith(\".csv\")]\n",
    "\n",
    "# Load each CSV file and concatenate them into one DataFrame\n",
    "dataframes = [pd.read_csv(file) for file in csv_files]\n",
    "combined_trained_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "combined_trained_df.to_pickle('preprocessed_train_nc_np.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*PREPROCESSING ALL DATA AND SAVING IT*\n",
    "\n",
    "Use this to make weights and tfidf, Could be more efficient. But eitherway nice to have preprocessed df saved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 285804/285804 [00:01<00:00, 182248.56it/s]\n",
      "Pandas Apply: 100%|██████████| 285804/285804 [00:04<00:00, 58128.90it/s] \n",
      "Pandas Apply: 100%|██████████| 285804/285804 [01:27<00:00, 3264.10it/s]\n",
      "Pandas Apply: 100%|██████████| 45024/45024 [00:00<00:00, 292213.81it/s]\n",
      "Pandas Apply: 100%|██████████| 45024/45024 [00:00<00:00, 349161.49it/s]\n",
      "Pandas Apply: 100%|██████████| 45024/45024 [00:13<00:00, 3291.55it/s]\n",
      "Pandas Apply: 100%|██████████| 628698/628698 [00:02<00:00, 258382.12it/s]\n",
      "Pandas Apply: 100%|██████████| 628698/628698 [00:01<00:00, 354098.91it/s]\n",
      "Pandas Apply: 100%|██████████| 628698/628698 [03:05<00:00, 3390.83it/s]\n",
      "Pandas Apply: 100%|██████████| 113402/113402 [00:01<00:00, 64915.40it/s]\n",
      "Pandas Apply: 100%|██████████| 113402/113402 [00:00<00:00, 431383.00it/s]\n",
      "Pandas Apply: 100%|██████████| 113402/113402 [00:32<00:00, 3486.78it/s]\n"
     ]
    }
   ],
   "source": [
    "#####PREPROCESSING TRAIN DATA AND SAVING IT#####\n",
    "\n",
    "# # Directories\n",
    "input_folder = \"eval_tweets\"\n",
    "output_folder = \"eval_tweets_original_preprocess\"\n",
    "os.makedirs(output_folder, exist_ok=True)  # Ensure the output folder exists\n",
    "\n",
    "# Process each file in the input folder\n",
    "csv_files = [os.path.join(input_folder, f) for f in os.listdir(input_folder) if f.endswith(\".csv\")]\n",
    "for file_path in csv_files:\n",
    "    # Load the CSV\n",
    "    current_df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Extract teams from filename\n",
    "    filename = os.path.basename(file_path)\n",
    "    home_team, away_team = extract_teams_from_filename(filename) \n",
    "    current_df['HomeTeam'] = home_team\n",
    "    current_df['AwayTeam'] = away_team\n",
    "    \n",
    "# Extract mentions from each tweet using swifter\n",
    "    current_df['Mentions'] = current_df['Tweet'].swifter.apply(extract_mentions)\n",
    "    current_df['hashtags'] = current_df['Tweet'].swifter.apply(extract_mentions)\n",
    "    current_df['is_RT'] = current_df['Tweet'].str.startswith('RT')\n",
    "    \n",
    "    # Preprocess the tweets\n",
    "    current_df['Tweet'] = current_df.swifter.apply(lambda row: preprocess_text(row['Tweet']), axis=1)\n",
    "    \n",
    "    # Save the preprocessed data to the output folder\n",
    "    output_path = os.path.join(output_folder, filename)\n",
    "    current_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 285804/285804 [00:00<00:00, 496188.55it/s]\n",
      "Pandas Apply: 100%|██████████| 45024/45024 [00:00<00:00, 419867.76it/s]\n",
      "Pandas Apply: 100%|██████████| 628698/628698 [00:01<00:00, 550472.98it/s]\n",
      "Pandas Apply: 100%|██████████| 113402/113402 [00:00<00:00, 546365.67it/s]\n"
     ]
    }
   ],
   "source": [
    "# # Directories\n",
    "input_folder = \"eval_tweets_original_preprocess\"\n",
    "output_folder = \"eval_tweets_original_preprocess_with_features\"\n",
    "os.makedirs(output_folder, exist_ok=True)  # Ensure the output folder exists\n",
    "\n",
    "# Process each file in the input folder\n",
    "csv_files = [os.path.join(input_folder, f) for f in os.listdir(input_folder) if f.endswith(\".csv\")]\n",
    "for file_path in csv_files:\n",
    "    # Load the CSV\n",
    "    current_df = pd.read_csv(file_path)\n",
    "    filename = os.path.basename(file_path)\n",
    "    \n",
    "    current_df['TweetLength'] = current_df['Tweet'].apply(len)\n",
    "    current_df['TweetCount'] = current_df.groupby(['MatchID', 'PeriodID', 'Timestamp'])['Timestamp'].transform('count')\n",
    "    current_df['WordCount'] = current_df['Tweet'].swifter.apply(lambda x: len(x.split()))\n",
    "    \n",
    "    output_path = os.path.join(output_folder, filename)\n",
    "    current_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 285804/285804 [00:20<00:00, 13747.56it/s]\n",
      "Pandas Apply: 100%|██████████| 45024/45024 [00:03<00:00, 13668.73it/s]\n",
      "Pandas Apply: 100%|██████████| 628698/628698 [00:38<00:00, 16250.83it/s]\n",
      "Pandas Apply: 100%|██████████| 113402/113402 [00:08<00:00, 13281.84it/s]\n"
     ]
    }
   ],
   "source": [
    "##### PREPROCESSING TRAIN DATA AND SAVING IT #####\n",
    "\n",
    "# Directories\n",
    "input_folder = \"eval_tweets_original_preprocess_with_features\"\n",
    "output_folder = \"eval_tweets_no_country\"\n",
    "os.makedirs(output_folder, exist_ok=True)  # Ensure the output folder exists\n",
    "\n",
    "# Process each file in the input folder\n",
    "csv_files = [os.path.join(input_folder, f) for f in os.listdir(input_folder) if f.endswith(\".csv\")]\n",
    "for file_path in csv_files:\n",
    "    # Load the CSV\n",
    "    current_df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Preprocess the 'Tweet' column\n",
    "    current_df['Tweet'] = current_df.swifter.apply(\n",
    "        lambda row: normalize_countries_with_teams(row['Tweet'], row['HomeTeam'], row['AwayTeam']), axis=1\n",
    "    )\n",
    "\n",
    "    # Save the processed DataFrame to the output folder\n",
    "    output_path = os.path.join(output_folder, os.path.basename(file_path))\n",
    "    current_df.to_csv(output_path, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: eval_tweets_no_country/GermanyGhana32.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 285804/285804 [00:04<00:00, 70072.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: eval_tweets_preprocessed_no_country_no_player/GermanyGhana32.csv\n",
      "Processing: eval_tweets_no_country/GermanySerbia2010.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 45024/45024 [00:00<00:00, 81115.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: eval_tweets_preprocessed_no_country_no_player/GermanySerbia2010.csv\n",
      "Processing: eval_tweets_no_country/NetherlandsMexico64.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 628698/628698 [00:07<00:00, 80921.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: eval_tweets_preprocessed_no_country_no_player/NetherlandsMexico64.csv\n",
      "Processing: eval_tweets_no_country/GreeceIvoryCoast44.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 113402/113402 [00:01<00:00, 82763.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: eval_tweets_preprocessed_no_country_no_player/GreeceIvoryCoast44.csv\n"
     ]
    }
   ],
   "source": [
    "##### PREPROCESSING TRAIN DATA AND SAVING IT #####\n",
    "# Load mentions with preprocessed mentions\n",
    "mentions_with_is_name = pd.read_pickle(\"mentions_processed.pkl\")\n",
    "combined_player_names = set(mentions_with_is_name[\"PreprocessedMention\"].dropna().unique())  # Use set for faster lookup\n",
    "\n",
    "# Directories\n",
    "input_folder = \"eval_tweets_no_country\"\n",
    "output_folder = \"eval_tweets_preprocessed_no_country_no_player\"\n",
    "os.makedirs(output_folder, exist_ok=True)  # Ensure the output folder exists\n",
    "\n",
    "\n",
    "# Process each file in the input folder\n",
    "csv_files = [os.path.join(input_folder, f) for f in os.listdir(input_folder) if f.endswith(\".csv\")]\n",
    "for file_path in csv_files:\n",
    "    print(f\"Processing: {file_path}\")\n",
    "    current_df = pd.read_csv(file_path)\n",
    "\n",
    "    # Apply the remove_player_names function\n",
    "    current_df['Tweet'] = current_df.swifter.apply(\n",
    "        lambda row: remove_player_names(row, combined_player_names), axis=1\n",
    "    )\n",
    "    \n",
    "    # Save the preprocessed data to the output folder\n",
    "    output_path = os.path.join(output_folder, os.path.basename(file_path))\n",
    "    current_df.to_csv(output_path, index=False)\n",
    "    print(f\"Saved: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the preprocessed CSV files\n",
    "input_folder = \"eval_tweets_original_preprocess_with_features\"\n",
    "\n",
    "# List all CSV files in the input folder\n",
    "csv_files = [os.path.join(input_folder, f) for f in os.listdir(input_folder) if f.endswith(\".csv\")]\n",
    "\n",
    "# Load each CSV file and concatenate them into one DataFrame\n",
    "dataframes = [pd.read_csv(file) for file in csv_files]\n",
    "combined_trained_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "combined_trained_df.to_pickle('preprocessed_eval_original.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the preprocessed CSV files\n",
    "input_folder = \"eval_tweets_no_country\"\n",
    "\n",
    "# List all CSV files in the input folder\n",
    "csv_files = [os.path.join(input_folder, f) for f in os.listdir(input_folder) if f.endswith(\".csv\")]\n",
    "\n",
    "# Load each CSV file and concatenate them into one DataFrame\n",
    "dataframes = [pd.read_csv(file) for file in csv_files]\n",
    "combined_trained_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "combined_trained_df.to_pickle('preprocessed_eval_no_country.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the preprocessed CSV files\n",
    "input_folder = \"eval_tweets_preprocessed_no_country_no_player\"\n",
    "\n",
    "# List all CSV files in the input folder\n",
    "csv_files = [os.path.join(input_folder, f) for f in os.listdir(input_folder) if f.endswith(\".csv\")]\n",
    "\n",
    "# Load each CSV file and concatenate them into one DataFrame\n",
    "dataframes = [pd.read_csv(file) for file in csv_files]\n",
    "combined_trained_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "combined_trained_df.to_pickle('preprocessed_eval_nc_np.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the two pickle files into separate DataFrames\n",
    "train_df = pd.read_pickle('preprocessed_train_original.pkl')\n",
    "eval_df = pd.read_pickle('preprocessed_eval_original.pkl')\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "full_df = pd.concat([train_df, eval_df], ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame as a new pickle file\n",
    "full_df.to_pickle('preprocessed_full_original.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the two pickle files into separate DataFrames\n",
    "train_df = pd.read_pickle('preprocessed_train_no_country.pkl')\n",
    "eval_df = pd.read_pickle('preprocessed_eval_no_country.pkl')\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "full_df = pd.concat([train_df, eval_df], ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame as a new pickle file\n",
    "full_df.to_pickle('preprocessed_full_no_country.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the two pickle files into separate DataFrames\n",
    "train_df = pd.read_pickle('preprocessed_train_nc_np.pkl')\n",
    "eval_df = pd.read_pickle('preprocessed_eval_nc_np.pkl')\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "full_df = pd.concat([train_df, eval_df], ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame as a new pickle file\n",
    "full_df.to_pickle('preprocessed_full_nc_np.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Weighted Average Embeddings with TF-IDF\n",
    "In this section, we calculate weighted average embeddings for tweets using pre-trained word embeddings and TF-IDF weights. This approach ensures that each word in a tweet contributes to the overall representation based on its importance (TF-IDF score). The resulting embedding is a single vector that captures the semantic meaning of the entire tweet, which can be used as features for downstream tasks such as classification, clustering, or similarity analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "\n",
    "full_df = pd.read_pickle('preprocessed_full_nc_np.pkl')\n",
    "\n",
    "# Optional: start from zero and fit on tweets\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer_all_tweets = TfidfVectorizer(\n",
    "    max_features=15000,           # Limit vocabulary size to 10,000 terms\n",
    "    min_df=3,                     # Ignore terms in fewer than 3 documents\n",
    "    #max_df=0.90,                  # Ignore overly frequent terms\n",
    "    sublinear_tf=True,            # Apply logarithmic scaling to term frequencies     \n",
    "    norm='l2',                    # L2 normalization\n",
    ") \n",
    "\n",
    "# Fit the TF-IDF vectorizer on minute-level documents\n",
    "vectorizer_all_tweets.fit(full_df['Tweet'])\n",
    "\n",
    "# Save the vectorizer\n",
    "with open(\"tfidf_vectorizer_all_tweets_nc_np.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vectorizer_all_tweets, f)\n",
    "\n",
    "# Load pre-computed\n",
    "with open('tfidf_vectorizer_all_tweets_nc_np.pkl', 'rb') as f:\n",
    "   vectorizer_all_tweets = pickle.load(f)\n",
    "\n",
    "# Extract TF-IDF weights\n",
    "tfidf_weights_all_tweets = dict(zip(vectorizer_all_tweets.get_feature_names_out(), vectorizer_all_tweets.idf_))\n",
    "\n",
    "\n",
    "# Weighted average embeddings\n",
    "def get_weighted_avg_embedding(tweet, model, vector_size=200, weights=tfidf_weights_all_tweets):\n",
    "    words = tweet.split()\n",
    "    word_vectors = [model[word] * weights.get(word, 1) for word in words if word in model]\n",
    "    if not word_vectors:\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean(word_vectors, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply:  73%|███████▎  | 3624443/4961848 [05:12<01:34, 14115.26it/s]"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for each tweet\n",
    "loaded_df = pd.read_pickle('preprocessed_train_nc_np.pkl')\n",
    "\n",
    "# vector_size = 200  # GloVe embedding dimension\n",
    "tweet_vectors = loaded_df['Tweet'].swifter.apply(lambda tweet: get_weighted_avg_embedding(tweet, model=glove_model, vector_size=200, weights=tfidf_weights_all_tweets))\n",
    "tweet_vectors = np.array(list(tweet_vectors), dtype=np.float32)\n",
    "\n",
    "# Save the tweet vectors\n",
    "with open(\"tweet_vectors_all_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tweet_vectors, f)\n",
    "\n",
    "print(\"Embeddings saved successfully!\")\n",
    "\n",
    "# Load the tweet vectors\n",
    "with open(\"tweet_vectors_all_data.pkl\", \"rb\") as f:\n",
    "    loaded_tweet_vectors = pickle.load(f)\n",
    "\n",
    "print(\"Embeddings loaded successfully!\")\n",
    "print(\"Loaded vectors shape:\", loaded_tweet_vectors.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Use if no period features ######\n",
    "tweet_df = pd.DataFrame(loaded_tweet_vectors)\n",
    "\n",
    "# Attach the vectors into the original dataframe\n",
    "period_features = pd.concat([loaded_df, tweet_df], axis=1)\n",
    "\n",
    "# Drop the columns that are not useful anymore\n",
    "period_features = period_features.drop(columns=['Timestamp', 'Tweet', 'HomeTeam', 'AwayTeam', 'HomeTeamCode', 'AwayTeamCode'])\n",
    "\n",
    "print(\"X_train_reshaped shape:\", period_features.shape)\n",
    "# Group the tweets into their corresponding periods. This way we generate an average embedding vector for each period\n",
    "period_features = period_features.groupby(['MatchID', 'PeriodID', 'ID']).mean().reset_index()\n",
    "\n",
    "# Save the tweet vectors\n",
    "with open(\"period_features.pkl\", \"wb\") as f:\n",
    "    pickle.dump(period_features, f)\n",
    "\n",
    "print(\"Period features saved successfully!\")\n",
    "\n",
    "# Load the tweet vectors\n",
    "with open(\"period_features.pkl\", \"rb\") as f:\n",
    "    loaded_period_features = pickle.load(f)\n",
    "\n",
    "print(\"Period features loaded successfully!\")\n",
    "print(\"Loaded vectors shape:\", loaded_period_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We drop the non-numerical features and keep the embeddings values for each period\n",
    "X = loaded_period_features.drop(columns=['EventType', 'MatchID', 'ID']).values\n",
    "# We extract the labels of our training samples\n",
    "y = loaded_period_features['EventType'].values\n",
    "\n",
    "\n",
    "# One-hot encode labels\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_encoded = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=SEED)\n",
    "\n",
    "# Add a time step dimension to match the LSTM input shape\n",
    "X_train_reshaped = X_train[:, None, :]  # Add a new axis for timesteps\n",
    "X_test_reshaped = X_test[:, None, :]    # Add a new axis for timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the early stopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',      # Monitor validation loss\n",
    "    patience=5,              # Stop training if no improvement after 3 epochs\n",
    "    restore_best_weights=True  # Restore the best weights when stopping\n",
    ")\n",
    "\n",
    "# Define the LSTM model with deterministic initializers\n",
    "model = Sequential([\n",
    "    tf.keras.layers.Input(shape=(1, X_train_reshaped.shape[2])),  \n",
    "    LSTM(\n",
    "        128, \n",
    "        return_sequences=False, \n",
    "        kernel_initializer=GlorotUniform(seed=SEED), \n",
    "        recurrent_initializer=Orthogonal(seed=SEED),\n",
    "        bias_initializer='zeros'\n",
    "    ),             \n",
    "    Dense(y_encoded.shape[1], activation='softmax', kernel_initializer=GlorotUniform(seed=SEED))\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_reshaped, y_train,\n",
    "                    epochs=50,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[early_stopping],  # Include the early stopping callback\n",
    "                    verbose=1)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test_reshaped, y_test, verbose=1)\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = pd.read_pickle('preprocessed_eval.pkl')\n",
    "print(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### For Kaggle submission\n",
    "\n",
    "predictions = []\n",
    "dummy_predictions = []\n",
    "# We read each file separately, we preprocess the tweets and then use the classifier to predict the labels.\n",
    "# Finally, we concatenate all predictions into a list that will eventually be concatenated and exported\n",
    "# to be submitted on Kaggle.\n",
    "\n",
    "val_df = pd.read_pickle('preprocessed_eval_nc_np.pkl')\n",
    "\n",
    "tweet_vectors = val_df['Tweet'].swifter.apply(lambda tweet: get_weighted_avg_embedding(tweet, model=glove_model, vector_size=200, weights= tfidf_weights_all_tweets))\n",
    "\n",
    "tweet_vectors = np.array(list(tweet_vectors), dtype=np.float32)\n",
    "\n",
    "tweet_df = pd.DataFrame(tweet_vectors)\n",
    "\n",
    "period_features_val = pd.concat([val_df, tweet_df], axis=1)\n",
    "period_features_val = period_features_val.drop(columns=['Timestamp', 'Tweet', 'HomeTeam', 'AwayTeam', 'HomeTeamCode', 'AwayTeamCode'])\n",
    "period_features_val = period_features_val.groupby(['MatchID', 'PeriodID', 'ID']).mean().reset_index()\n",
    "\n",
    "X = period_features_val.drop(columns=['MatchID', 'ID']).values\n",
    "\n",
    "# Reshape input for LSTM\n",
    "X_reshaped = X[:, None, :]  # Add timestep dimension\n",
    "\n",
    "preds = model.predict(X_reshaped)\n",
    "preds = preds.argmax(axis=1)  # Convert probabilities to class indices\n",
    "period_features_val['EventType'] = preds\n",
    "predictions.append(period_features_val[['ID', 'EventType']])\n",
    "\n",
    "\n",
    "pred_df = pd.concat(predictions)\n",
    "pred_df.to_csv('LSTM_predictions.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Save TF-IDF trained on the training data\n",
    "with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_results = remove_rows_with_names(chi2_results, name_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chi2_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = chi2_results[:62]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for the chi² scores\n",
    "chi2_dict = dict(zip(top_words['Word'], top_words['Chi2_Score']))\n",
    "\n",
    "# Compute weighted score features for each important word\n",
    "for word in chi2_dict.keys():\n",
    "    loaded_df[f'WeightedScore_{word}'] = loaded_df['Tweet'].apply(\n",
    "        lambda tweet: tweet.split().count(word) * chi2_dict.get(word, 0)\n",
    "    )\n",
    "\n",
    "print(\"loaded_df shape:\", loaded_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_df.to_pickle('preprocessed_train_extra_weights.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = loaded_df[['TweetLength', 'WordCount']].corr()\n",
    "print(\"Correlation between TweetLength and WordCount:\")\n",
    "print(correlation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average word length in a tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the mentions DataFrame\n",
    "mentions = pd.read_pickle(\"mentions_with_teams.pkl\")\n",
    "\n",
    "# Filter mentions based on their occurrences in a specific HomeTeam and AwayTeam combination\n",
    "mentions['mention_count'] = mentions.groupby(['HomeTeam', 'AwayTeam', 'Mention'])['Mention'].transform('count')\n",
    "mentions = mentions[mentions['mention_count'] > 10]\n",
    "\n",
    "# Drop duplicates\n",
    "mentions = mentions.drop_duplicates()\n",
    "\n",
    "# Load the last names DataFrame\n",
    "last_names_df = pd.read_csv(\"last_names.csv\")\n",
    "\n",
    "# Ensure the last names DataFrame contains 'last_name' and 'team' columns\n",
    "if {'last_name', 'nationality'}.issubset(last_names_df.columns):\n",
    "    # Add the 'is_name' column\n",
    "    def is_name(mention, home_team, away_team):\n",
    "        # Filter last names for the HomeTeam and AwayTeam\n",
    "        team_last_names = last_names_df[last_names_df['nationality'].isin([home_team, away_team])]['last_name'].str.lower().tolist()\n",
    "        # Check if any last name from these teams matches the mention\n",
    "        return 1 if any(last_name in mention.lower() for last_name in team_last_names) else 0\n",
    "\n",
    "    # Apply the function row-wise using swifter\n",
    "    mentions['is_name'] = mentions.swifter.apply(\n",
    "        lambda row: is_name(row['Mention'], row['HomeTeam'], row['AwayTeam']), axis=1\n",
    "    )\n",
    "\n",
    "    # Drop the temporary mention_count column\n",
    "    mentions.drop(columns=['mention_count'], inplace=True)\n",
    "\n",
    "    # Print the updated DataFrame\n",
    "    print(mentions)\n",
    "\n",
    "    # Optionally, save the updated DataFrame to a new pickle file\n",
    "    mentions.to_pickle(\"mentions_with_is_name.pkl\")\n",
    "\n",
    "else:\n",
    "    print(\"The last_names.csv file must contain 'last_name' and 'team' columns.\")\n",
    "    \n",
    "    \n",
    "\n",
    "# Load the DataFrame\n",
    "mentions_with_is_name = pd.read_pickle(\"mentions_with_is_name.pkl\")\n",
    "\n",
    "# Convert mentions to lowercase\n",
    "mentions_with_is_name['Mention'] = mentions_with_is_name['Mention']\n",
    "\n",
    "mentions_with_is_name = mentions_with_is_name[mentions_with_is_name['is_name'] != 0]\n",
    "\n",
    "\n",
    "mentions_with_is_name = mentions_with_is_name[~mentions_with_is_name['Mention'].str.contains('Brazil', na=False)]\n",
    "mentions_with_is_name = mentions_with_is_name[~mentions_with_is_name['Mention'].str.contains('soccer', na=False)]\n",
    "mentions_with_is_name = mentions_with_is_name[~mentions_with_is_name['Mention'].str.contains('ball', na=False)]\n",
    "mentions_with_is_name = mentions_with_is_name[~mentions_with_is_name['Mention'].str.contains('Twitter', na=False)]\n",
    "\n",
    "\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(mentions_with_is_name)\n",
    "\n",
    "# Optionally, save the updated DataFrame to a new pickle file\n",
    "mentions_with_is_name.to_pickle(\"mentions_without_football.pkl\")\n",
    "\n",
    "mentions_with_is_name = pd.read_pickle(\"mentions_without_football.pkl\")\n",
    "mentions_with_is_name['PreprocessedMention'] = mentions_with_is_name.swifter.apply(\n",
    "    lambda row: preprocess_text(row['Mention'], row['HomeTeam'], row['AwayTeam']), axis=1\n",
    ")\n",
    "mentions_with_is_name.to_pickle(\"mentions_processed.pkl\")\n",
    "print(mentions_with_is_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Directories\n",
    "\n",
    "##### PREPROCESSING TRAIN DATA AND SAVING IT #####\n",
    "import os\n",
    "import pandas as pd\n",
    "import swifter\n",
    "\n",
    "# Load mentions with preprocessed mentions\n",
    "mentions_with_is_name = pd.read_pickle(\"mentions_processed.pkl\")\n",
    "combined_player_names = set(mentions_with_is_name[\"PreprocessedMention\"].dropna().unique())  # Use set for faster lookup\n",
    "\n",
    "# Directories\n",
    "input_folder = \"eval_tweets_preprocessed\"\n",
    "output_folder = \"eval_tweets_preprocessed_no_player\"\n",
    "os.makedirs(output_folder, exist_ok=True)  # Ensure the output folder exists\n",
    "\n",
    "# Define the remove_player_names function\n",
    "\n",
    "# Process each file in the input folder\n",
    "csv_files = [os.path.join(input_folder, f) for f in os.listdir(input_folder) if f.endswith(\".csv\")]\n",
    "for file_path in csv_files:\n",
    "    print(f\"Processing: {file_path}\")\n",
    "    current_df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Apply the remove_player_names function\n",
    "    current_df['Tweet'] = current_df.swifter.apply(\n",
    "        lambda row: remove_player_names(row, combined_player_names), axis=1\n",
    "    )\n",
    "    \n",
    "    # Save the preprocessed data to the output folder\n",
    "    output_path = os.path.join(output_folder, os.path.basename(file_path))\n",
    "    current_df.to_csv(output_path, index=False)\n",
    "    print(f\"Saved: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
