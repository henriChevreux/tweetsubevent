{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-20 21:46:36.740374: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/henrichevreux/opt/anaconda3/envs/inf554/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import gensim.downloader as api\n",
    "import re\n",
    "import swifter\n",
    "\n",
    "# Enable tqdm for pandas\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the average word vector for a tweet\n",
    "def get_avg_embedding(tweet, model, vector_size=200):\n",
    "    words = tweet.split()  # Tokenize by whitespace\n",
    "    word_vectors = [model[word] for word in words if word in model]\n",
    "    if not word_vectors:  # If no words in the tweet are in the vocabulary, return a zero vector\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Tokenization\n",
    "    words = text.split()\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: Transformer embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 86843/86843 [00:15<00:00, 5457.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of           ID  MatchID  PeriodID  EventType      Timestamp  \\\n",
      "0        2_0        2         0          0  1403538600000   \n",
      "1        2_0        2         0          0  1403538600000   \n",
      "2        2_0        2         0          0  1403538600000   \n",
      "3        2_0        2         0          0  1403538600000   \n",
      "4        2_0        2         0          0  1403538600000   \n",
      "...      ...      ...       ...        ...            ...   \n",
      "86838  2_129        2       129          1  1403546400000   \n",
      "86839  2_129        2       129          1  1403546400000   \n",
      "86840  2_129        2       129          1  1403546400000   \n",
      "86841  2_129        2       129          1  1403546400000   \n",
      "86842  2_129        2       129          1  1403546400000   \n",
      "\n",
      "                                                   Tweet  \n",
      "0      rt soccerdotcom esp beat au well give away spa...  \n",
      "1      visit sitep official web site httptcoehzkslan ...  \n",
      "2      rt soccerdotcom esp beat au well give away spa...  \n",
      "3      rt worldsoccershop winner au v esp match well ...  \n",
      "4      rt soccerdotcom au beat esp well give away aus...  \n",
      "...                                                  ...  \n",
      "86838  rt soccerdotcom final au esp spain end worldcu...  \n",
      "86839  rt fifaworldcup ft au esp guajevilla torres ju...  \n",
      "86840  end road au esp catching early flight back hom...  \n",
      "86841  rt footballfact worldcup group b final standin...  \n",
      "86842  rt easportsfifa ned top group b perfect record...  \n",
      "\n",
      "[86843 rows x 6 columns]>\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "folder_path = \"train_tweets\"\n",
    "csv_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(\".csv\")][:1] # Only use first file for testing purposes\n",
    "df = pd.concat((pd.read_csv(f) for f in csv_files), ignore_index=True)\n",
    "\n",
    "# Apply preprocessing\n",
    "df['Tweet'] = df['Tweet'].swifter.apply(preprocess_text)\n",
    "print(df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe model\n",
    "glove_model = api.load(\"glove-twitter-200\")  # 200-dimensional GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 86843/86843 [00:03<00:00, 22304.41it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for each tweet\n",
    "vector_size = 200  # GloVe embedding dimension\n",
    "tweet_vectors = df['Tweet'].swifter.apply(lambda x: get_avg_embedding(x, glove_model, vector_size))\n",
    "tweet_vectors = np.array(list(tweet_vectors), dtype=np.float32)\n",
    "\n",
    "#tweet_vectors = df['Tweet'].swifter.apply(get_tweet_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_reshaped shape: (86843, 204)\n",
      "Epoch 1/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 306ms/step - accuracy: 0.4804 - loss: 0.6992 - val_accuracy: 0.7619 - val_loss: 0.6639\n",
      "Epoch 2/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.5702 - loss: 0.6793 - val_accuracy: 0.7619 - val_loss: 0.6241\n",
      "Epoch 3/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.5702 - loss: 0.6639 - val_accuracy: 0.7619 - val_loss: 0.5973\n",
      "Epoch 4/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.5390 - loss: 0.6610 - val_accuracy: 0.7619 - val_loss: 0.5788\n",
      "Epoch 5/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.5650 - loss: 0.6415 - val_accuracy: 0.7619 - val_loss: 0.5596\n",
      "Epoch 6/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.5858 - loss: 0.6266 - val_accuracy: 0.7619 - val_loss: 0.5468\n",
      "Epoch 7/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.5806 - loss: 0.6222 - val_accuracy: 0.7619 - val_loss: 0.5408\n",
      "Epoch 8/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.5754 - loss: 0.6117 - val_accuracy: 0.7619 - val_loss: 0.5377\n",
      "Epoch 9/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.6766 - loss: 0.6080 - val_accuracy: 0.8571 - val_loss: 0.5348\n",
      "Epoch 10/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8010 - loss: 0.5895 - val_accuracy: 0.8571 - val_loss: 0.5286\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6538 - loss: 0.6261\n",
      "Test Accuracy: 0.6538\n"
     ]
    }
   ],
   "source": [
    "tweet_df = pd.DataFrame(tweet_vectors)\n",
    "\n",
    "# Attach the vectors into the original dataframe\n",
    "period_features = pd.concat([df, tweet_df], axis=1)\n",
    "# Drop the columns that are not useful anymore\n",
    "period_features = period_features.drop(columns=['Timestamp', 'Tweet'])\n",
    "\n",
    "print(\"X_train_reshaped shape:\", period_features.shape)\n",
    "# Group the tweets into their corresponding periods. This way we generate an average embedding vector for each period\n",
    "period_features = period_features.groupby(['MatchID', 'PeriodID', 'ID']).mean().reset_index()\n",
    "\n",
    "# We drop the non-numerical features and keep the embeddings values for each period\n",
    "X = period_features.drop(columns=['EventType', 'MatchID', 'PeriodID', 'ID']).values\n",
    "# We extract the labels of our training samples\n",
    "y = period_features['EventType'].values\n",
    "\n",
    "\n",
    "# One-hot encode labels\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_encoded = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Add a time step dimension to match the LSTM input shape\n",
    "X_train_reshaped = X_train[:, None, :]  # Add a new axis for timesteps\n",
    "X_test_reshaped = X_test[:, None, :]    # Add a new axis for timesteps\n",
    "\n",
    "\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential([\n",
    "    tf.keras.layers.Input(shape=(1, vector_size)),  # Input layer\n",
    "    LSTM(128, return_sequences=False),             # LSTM layer with 128 units\n",
    "    Dense(y_encoded.shape[1], activation='softmax')  # Output layer\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_reshaped, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=64,\n",
    "                    validation_split=0.2,\n",
    "                    verbose=1)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test_reshaped, y_test, verbose=1)\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 285804/285804 [00:50<00:00, 5627.99it/s]\n",
      "Pandas Apply: 100%|██████████| 285804/285804 [00:09<00:00, 29237.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 45024/45024 [00:07<00:00, 5861.59it/s]\n",
      "Pandas Apply: 100%|██████████| 45024/45024 [00:01<00:00, 26142.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 628698/628698 [01:43<00:00, 6070.59it/s]\n",
      "Pandas Apply: 100%|██████████| 628698/628698 [00:19<00:00, 31739.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 113402/113402 [00:18<00:00, 6202.32it/s]\n",
      "Pandas Apply: 100%|██████████| 113402/113402 [00:03<00:00, 31547.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n"
     ]
    }
   ],
   "source": [
    "###### For Kaggle submission\n",
    "\n",
    "predictions = []\n",
    "dummy_predictions = []\n",
    "# We read each file separately, we preprocess the tweets and then use the classifier to predict the labels.\n",
    "# Finally, we concatenate all predictions into a list that will eventually be concatenated and exported\n",
    "# to be submitted on Kaggle.\n",
    "for fname in os.listdir(\"eval_tweets\"):\n",
    "    val_df = pd.read_csv(\"eval_tweets/\" + fname)\n",
    "    \n",
    "    val_df['Tweet'] = val_df['Tweet'].swifter.apply(preprocess_text)\n",
    "\n",
    "    tweet_vectors = val_df['Tweet'].swifter.apply(lambda x: get_avg_embedding(x, glove_model, vector_size))\n",
    "\n",
    "    tweet_vectors = np.array(list(tweet_vectors), dtype=np.float32)\n",
    "\n",
    "    tweet_df = pd.DataFrame(tweet_vectors)\n",
    "\n",
    "    period_features = pd.concat([val_df, tweet_df], axis=1)\n",
    "    period_features = period_features.drop(columns=['Timestamp', 'Tweet'])\n",
    "    period_features = period_features.groupby(['MatchID', 'PeriodID', 'ID']).mean().reset_index()\n",
    "\n",
    "    X = period_features.drop(columns=['MatchID', 'PeriodID', 'ID']).values\n",
    "\n",
    "    # Reshape input for LSTM\n",
    "    X_reshaped = X[:, None, :]  # Add timestep dimension\n",
    "\n",
    "    preds = model.predict(X_reshaped)\n",
    "    preds = preds.argmax(axis=1)  # Convert probabilities to class indices\n",
    "    period_features['EventType'] = preds\n",
    "    predictions.append(period_features[['ID', 'EventType']])\n",
    "\n",
    "\n",
    "pred_df = pd.concat(predictions)\n",
    "pred_df.to_csv('LSTM_predictions.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inf554",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
