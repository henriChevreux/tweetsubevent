{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-11 13:28:40.593488: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import gensim.downloader as api\n",
    "import re\n",
    "import swifter\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import GlorotUniform, Orthogonal\n",
    "import random\n",
    "# Enable tqdm for pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# Ensure Reproducibility\n",
    "import random\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "\n",
    "# Python's built-in random\n",
    "random.seed(SEED)\n",
    "\n",
    "# NumPy\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# TensorFlow\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Set Python hash seed\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "\n",
    "# Configure TensorFlow for deterministic operations\n",
    "tf.keras.utils.set_random_seed(SEED)  # Sets all random seeds for the program (Python, NumPy, and TensorFlow)\n",
    "tf.config.experimental.enable_op_determinism()  # Enable deterministic operations in TensorFlow\n",
    "\n",
    "# If using GPU, you might also want to set these:\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    # Force TensorFlow to use deterministic GPU operations\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "    # Limit GPU memory growth\n",
    "    for gpu in tf.config.experimental.list_physical_devices('GPU'):\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # Limit to one GPU if using multiple GPUs\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe model\n",
    "glove_model = api.load(\"glove-twitter-200\")  # 200-dimensional GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Country list for team extraction\n",
    "country_list = [\n",
    "    \"Argentina\", \"Belgium\", \"Germany\", \"Serbia\", \"Greece\", \"IvoryCoast\", \n",
    "    \"Netherlands\", \"Mexico\", \"Australia\", \"Spain\", \"SouthKorea\", \n",
    "    \"Cameroon\", \"Brazil\", \"France\", \"Nigeria\", \"Algeria\", \"USA\", \n",
    "    \"Honduras\", \"Switzerland\", \"Croatia\", \"Chile\", \"Portugal\", \n",
    "    \"Ghana\", \"Slovenia\"\n",
    "]\n",
    "\n",
    "country_variations = {\n",
    "    'argentina': ['argentina', 'arg', 'argentine', 'argies', 'albiceleste', 'argentinian', 'argentinos', 'argentinas'],\n",
    "    'australia': ['australia', 'aus', 'aussie', 'aussies', 'socceroos', 'oz', 'straya', 'australian', 'au'],\n",
    "    'belgium': ['belgium', 'bel', 'belgique', 'belgie', 'belgian', 'belgians', 'red devils', 'diables rouges'],\n",
    "    'brazil': ['brazil', 'bra', 'brasil', 'bresil', 'brazilian', 'brazilians', 'selecao', 'canarinho', 'verde amarela', 'samba boys'],\n",
    "    'cameroon': ['cameroon', 'cmr', 'cameroun', 'camerounais', 'indomitable lions', 'lions'],\n",
    "    'france': ['france', 'fra', 'french', 'les bleus', 'tricolore', 'tricolores', 'equipe de france', 'allez les bleus'],\n",
    "    'honduras': ['honduras', 'hon', 'honduran', 'hondurans', 'los catrachos', 'catrachos', 'la h'],\n",
    "    'portugal': ['portugal', 'por', 'portuguese', 'selecao das quinas', 'seleccao', 'navegadores', 'team portugal'],\n",
    "    'spain': ['spain', 'esp', 'espana', 'espania', 'spanish', 'la roja', 'furia roja', 'la furia', 'la seleccion'],\n",
    "    'southkorea': ['south korea', 'korea', 'kor', 'skorea', 'korean', 'koreans', 'taeguk warriors', 'warriors'],\n",
    "    'switzerland': ['switzerland', 'sui', 'suisse', 'schweiz', 'swiss', 'nati', 'rossocrociati', 'a team'],\n",
    "    'usa': ['usa', 'united states', 'america', 'united states of america', 'us', 'usa', 'usmnt', 'americans', 'american', 'yanks', 'uncle sam', 'stars and stripes', 'team usa'],\n",
    "    'ghana': ['ghana', 'gha', 'ghanaian', 'ghanaians', 'black stars', 'stars'],\n",
    "    'netherlands': ['netherlands', 'ned', 'holland', 'dutch', 'oranje', 'flying dutchmen', 'orange', 'clockwork orange', 'nederlands'],\n",
    "    'germany': ['germany', 'ger', 'alemania', 'deutschland', 'german', 'germans', 'die mannschaft', 'nationalelf', 'deu'],\n",
    "    'iran': ['iran', 'irn', 'iranian', 'iranians', 'team melli', 'persian stars'],\n",
    "    'nigeria': ['nigeria', 'nga', 'naija', 'super eagles', 'eagles', 'nigerian', 'nigerians', 'green eagles'],\n",
    "    'algeria': ['algeria', 'alg', 'algerian', 'algerians', 'fennecs', 'desert foxes', 'les verts'],\n",
    "    'croatia': ['croatia', 'cro', 'hrvatska', 'hrv', 'croatian', 'croatians', 'vatreni', 'blazers', 'kockasti'],\n",
    "    'chile': ['chile', 'chi', 'chilean', 'chileans', 'la roja', 'team chile'],\n",
    "    'slovenia': ['slovenia', 'svn', 'slovenian', 'slovenians', 'slovenski', 'boys'],\n",
    "    'serbia': ['serbia', 'srb', 'serbian', 'serbians', 'beli orlovi', 'white eagles', 'orlovi'],\n",
    "    'greece': ['greece', 'gre', 'greek', 'greeks', 'piratiko', 'ethniki', 'galanolefki'],\n",
    "    'ivorycoast': ['ivory coast', 'civ', 'cote divoire', 'cotedivoire', 'ivorians', 'les elephants', 'elephants', 'ivory'],\n",
    "    'mexico': ['mexico', 'mex', 'mexiko', 'mexican', 'mexicans', 'el tri', 'tricolor', 'aztecas', 'el tricolor', 'verde']\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the average word vector for a tweet\n",
    "def get_avg_embedding(tweet, model, vector_size=200):\n",
    "    words = tweet.split()  # Tokenize by whitespace\n",
    "    word_vectors = [model[word] for word in words if word in model]\n",
    "    if not word_vectors:  # If no words in the tweet are in the vocabulary, return a zero vector\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "def extract_teams_from_filename(filename):\n",
    "    # Remove numbers and file extension\n",
    "    base_name = re.sub(r'\\d+\\.csv$', '', filename)\n",
    "    \n",
    "    # Identify teams from the predefined country list\n",
    "    teams = [country for country in country_list if country in base_name]\n",
    "    \n",
    "    if len(teams) >= 2:\n",
    "        return teams[0], teams[1]\n",
    "    elif len(teams) == 1:\n",
    "        return teams[0], \"Unknown\"\n",
    "    else:\n",
    "        return \"Unknown\", \"Unknown\"\n",
    "    \n",
    "def normalize_countries_with_teams(words, home_team, away_team):\n",
    "    # Text is already preprocessed and split into words\n",
    "    normalized_words = words.copy()\n",
    "    \n",
    "    # Replace individual country mentions\n",
    "    for i, word in enumerate(words):\n",
    "        # Check if it's home team\n",
    "        if word in country_variations.get(home_team.lower(), []):\n",
    "            normalized_words[i] = 'hometeam'\n",
    "        # Check if it's away team\n",
    "        elif word in country_variations.get(away_team.lower(), []):\n",
    "            normalized_words[i] = 'awayteam'\n",
    "        else:\n",
    "            # Check if it's any other country\n",
    "            for country, variations in country_variations.items():\n",
    "                if word in variations and country.lower() not in [home_team.lower(), away_team.lower()]:\n",
    "                    normalized_words[i] = 'othercountry'\n",
    "                    break\n",
    "    \n",
    "    return normalized_words\n",
    "\n",
    "def normalize_football_player_names_with_country(words, country, football_player_data):\n",
    "    \"\"\"\n",
    "    Normalize football player names in a list of words to 'footballplayername', filtering by country.\n",
    "\n",
    "    Args:\n",
    "        words (list of str): List of words (preprocessed text).\n",
    "        country (str): Country code of the players to focus on.\n",
    "        football_player_data (dict): Dictionary mapping country codes to lists of last names.\n",
    "\n",
    "    Returns:\n",
    "        list of str: Words with football player names normalized to 'footballplayername'.\n",
    "    \"\"\"\n",
    "    normalized_words = words.copy()\n",
    "    \n",
    "    # Get the list of player names for the given country\n",
    "    player_last_names = football_player_data.get(country, [])\n",
    "    \n",
    "    # Replace individual football player last names\n",
    "    for i, word in enumerate(words):\n",
    "        if word in player_last_names:\n",
    "            normalized_words[i] = 'footballplayername'\n",
    "    \n",
    "    return normalized_words\n",
    "\n",
    "def preprocess_text(text, hometeam, awayteam):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Tokenization\n",
    "    words = text.split()\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    words = [word for word in words if not word.startswith('http')]\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    # Apply country normalization if matchID is provided\n",
    "    words = normalize_countries_with_teams(words, hometeam, awayteam)\n",
    "    \n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5056050/5056050 [11:03<00:00, 7622.41it/s] \n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "folder_path = \"train_tweets\"\n",
    "csv_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(\".csv\")]\n",
    "# First load all data with team information\n",
    "dataframes = []\n",
    "for file_path in csv_files:\n",
    "    # Load the CSV\n",
    "    current_df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Get filename and extract teams\n",
    "    filename = os.path.basename(file_path)\n",
    "    home_team, away_team = extract_teams_from_filename(filename)\n",
    "    \n",
    "    # Add team columns\n",
    "    current_df['HomeTeam'] = home_team\n",
    "    current_df['AwayTeam'] = away_team\n",
    "    \n",
    "    dataframes.append(current_df)\n",
    "\n",
    "# Concatenate all dataframes\n",
    "df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "\n",
    "# Then apply regular preprocessing with MatchID\n",
    "df['Tweet'] = df.swifter.apply(lambda row: preprocess_text(row['Tweet'], row['HomeTeam'], row['AwayTeam']), axis=1)\n",
    "\n",
    "df.to_pickle('preprocessed_train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()  # For progress tracking\n",
    "\n",
    "# Load the preprocessed last names grouped by country\n",
    "grouped_last_names_pickle_path = 'grouped_football_player_last_names.pkl'\n",
    "with open(grouped_last_names_pickle_path, 'rb') as pickle_file:\n",
    "    football_player_data = pickle.load(pickle_file)\n",
    "    \n",
    "\n",
    "# Define the country code mapping\n",
    "country_code_mapping = {\n",
    "    \"Argentina\": \"ar\", \"Belgium\": \"be\", \"Germany\": \"de\", \"Serbia\": \"rs\", \"Greece\": \"gr\",\n",
    "    \"IvoryCoast\": \"ci\", \"Netherlands\": \"nl\", \"Mexico\": \"mx\", \"Australia\": \"au\", \"Spain\": \"es\",\n",
    "    \"SouthKorea\": \"kr\", \"Cameroon\": \"cm\", \"Brazil\": \"br\", \"France\": \"fr\", \"Nigeria\": \"ng\",\n",
    "    \"Algeria\": \"dz\", \"USA\": \"us\", \"Honduras\": \"hn\", \"Switzerland\": \"ch\", \"Croatia\": \"hr\",\n",
    "    \"Chile\": \"cl\", \"Portugal\": \"pt\", \"Ghana\": \"gh\", \"Slovenia\": \"si\"\n",
    "}\n",
    "\n",
    "# Load preprocessed tweets with HomeTeam and AwayTeam information\n",
    "df = pd.read_pickle('preprocessed_train.pkl')\n",
    "\n",
    "# Function to map team names to country codes\n",
    "def map_team_to_country_code(team_name, country_code_mapping):\n",
    "    return country_code_mapping.get(team_name, \"unknown\")\n",
    "\n",
    "# Update HomeTeam and AwayTeam to their corresponding country codes\n",
    "df['HomeTeamCode'] = df['HomeTeam'].apply(lambda x: map_team_to_country_code(x, country_code_mapping))\n",
    "df['AwayTeamCode'] = df['AwayTeam'].apply(lambda x: map_team_to_country_code(x, country_code_mapping))\n",
    "\n",
    "def remove_player_names(row, football_player_data):\n",
    "    \"\"\"\n",
    "    Replace player names in the tweet with 'footballplayername'.\n",
    "    \"\"\"\n",
    "    home_team_code = row['HomeTeamCode']\n",
    "    away_team_code = row['AwayTeamCode']\n",
    "    \n",
    "    current_df['HomeTeam'] = home_team\n",
    "    current_df['AwayTeam'] = away_team\n",
    "    \n",
    "    # Combine player names for home and away teams\n",
    "    relevant_names = set(football_player_data.get(home_team_code, [])) | set(football_player_data.get(away_team_code, []))\n",
    "    \n",
    "    # Tokenize the tweet\n",
    "    words = row['Tweet'].split()\n",
    "    \n",
    "    # Normalize player names\n",
    "    normalized_words = [\n",
    "        'footballplayername' if word in relevant_names else word for word in words\n",
    "    ]\n",
    "    \n",
    "    return ' '.join(normalized_words)\n",
    "\n",
    "# Apply the normalization to the tweets\n",
    "df['Tweet'] = df.progress_apply(lambda row: remove_player_names(row, football_player_data), axis=1)\n",
    "\n",
    "# Save the updated DataFrame\n",
    "df.to_pickle('preprocessed_train_with_names_removed.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ID  MatchID  PeriodID  EventType      Timestamp  \\\n",
      "0  2_0        2         0          0  1403538600000   \n",
      "1  2_0        2         0          0  1403538600000   \n",
      "2  2_0        2         0          0  1403538600000   \n",
      "3  2_0        2         0          0  1403538600000   \n",
      "4  2_0        2         0          0  1403538600000   \n",
      "\n",
      "                                               Tweet   HomeTeam AwayTeam  \\\n",
      "0  rt soccerdotcom awayteam beat hometeam well gi...  Australia    Spain   \n",
      "1  visit sitep official web site spanis real stat...  Australia    Spain   \n",
      "2  rt soccerdotcom awayteam beat hometeam well gi...  Australia    Spain   \n",
      "3  rt worldsoccershop winner hometeam v awayteam ...  Australia    Spain   \n",
      "4  rt soccerdotcom hometeam beat awayteam well gi...  Australia    Spain   \n",
      "\n",
      "   AvgWordLength  TweetCount HomeTeamCode AwayTeamCode  TweetLength  WordCount  \n",
      "0       0.072464           7           au           es     0.340136         16  \n",
      "1       0.081781           7           au           es     0.309524         13  \n",
      "2       0.072464           7           au           es     0.340136         16  \n",
      "3       0.070759           7           au           es     0.333333         16  \n",
      "4       0.072464           7           au           es     0.340136         16  \n"
     ]
    }
   ],
   "source": [
    "# Feature creation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "MMscaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "loaded_df = pd.read_pickle('preprocessed_train_with_names_removed.pkl')\n",
    "\n",
    "# Add the length of each tweet as a feature\n",
    "loaded_df['TweetLength'] = loaded_df['Tweet'].apply(len)\n",
    "\n",
    "# Add a simple tweet count feature\n",
    "loaded_df['TweetCount'] = loaded_df.groupby(['MatchID', 'PeriodID', 'Timestamp'])['Timestamp'].transform('count')\n",
    "\n",
    "# Add word count as a feature\n",
    "loaded_df['WordCount'] = loaded_df['Tweet'].apply(lambda x: len(x.split()))\n",
    "\n",
    "loaded_df['TweetLength'] = MMscaler.fit_transform(loaded_df['TweetLength'].values.reshape(-1, 1))\n",
    "# Add 1 to avoid division by zero\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(loaded_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*PREPROCESSING ALL DATA AND SAVING IT*\n",
    "\n",
    "Use this to make weights and tfidf, Could be more efficient. But eitherway nice to have preprocessed df saved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a9c36af1b8d4b5db6aefec8f1ef9633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/285804 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'HomeTeam'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'HomeTeam'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 24\u001b[0m\n\u001b[1;32m     19\u001b[0m current_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTweet\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m current_df\u001b[38;5;241m.\u001b[39mswifter\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m row: preprocess_text(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTweet\u001b[39m\u001b[38;5;124m'\u001b[39m], home_team, away_team), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Update HomeTeam and AwayTeam to their corresponding country codes\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m current_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHomeTeamCode\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mcurrent_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHomeTeam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: map_team_to_country_code(x, country_code_mapping))\n\u001b[1;32m     25\u001b[0m current_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAwayTeamCode\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m current_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAwayTeam\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: map_team_to_country_code(x, country_code_mapping))\n\u001b[1;32m     27\u001b[0m current_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTweet\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m current_df\u001b[38;5;241m.\u001b[39mprogress_apply(\u001b[38;5;28;01mlambda\u001b[39;00m row: remove_player_names(row, football_player_data), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'HomeTeam'"
     ]
    }
   ],
   "source": [
    "#####PREPROCESSING ALL DATA AND SAVING IT#####\n",
    "\n",
    "# # Directories\n",
    "input_folder = \"eval_tweets\"\n",
    "output_folder = \"eval_tweets_preprocessed\"\n",
    "os.makedirs(output_folder, exist_ok=True)  # Ensure the output folder exists\n",
    "\n",
    "# Process each file in the input folder\n",
    "csv_files = [os.path.join(input_folder, f) for f in os.listdir(input_folder) if f.endswith(\".csv\")]\n",
    "for file_path in csv_files:\n",
    "    # Load the CSV\n",
    "    current_df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Extract teams from filename\n",
    "    filename = os.path.basename(file_path)\n",
    "    home_team, away_team = extract_teams_from_filename(filename)  # Use your existing function\n",
    "    \n",
    "    # Preprocess the tweets\n",
    "    current_df['Tweet'] = current_df.swifter.apply(\n",
    "        lambda row: preprocess_text(row['Tweet'], home_team, away_team), axis=1\n",
    "    )\n",
    "    \n",
    "    # Update HomeTeam and AwayTeam to their corresponding country codes\n",
    "    current_df['HomeTeamCode'] = current_df['HomeTeam'].apply(lambda x: map_team_to_country_code(x, country_code_mapping))\n",
    "    current_df['AwayTeamCode'] = current_df['AwayTeam'].apply(lambda x: map_team_to_country_code(x, country_code_mapping))\n",
    "\n",
    "    current_df['Tweet'] = current_df.progress_apply(lambda row: remove_player_names(row, football_player_data), axis=1)\n",
    "\n",
    "    # Save the preprocessed data to the output folder\n",
    "    output_path = os.path.join(output_folder, filename)\n",
    "    current_df.to_csv(output_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "\n",
    "# # Folder containing preprocessed tweets\n",
    "# input_folder = \"all_tweets_preprocessed\"\n",
    "\n",
    "# # Collect all tweets grouped by minute across all files\n",
    "# tweet_documents = []\n",
    "\n",
    "# # Process each preprocessed file\n",
    "# csv_files = [os.path.join(input_folder, f) for f in os.listdir(input_folder) if f.endswith(\".csv\")]\n",
    "# for file_path in csv_files:\n",
    "#     # Load the preprocessed CSV\n",
    "#     df_all_tweets = pd.read_csv(file_path)\n",
    "#     tweet_documents.extend(df_all_tweets['Tweet'].tolist())\n",
    "    \n",
    "\n",
    "# # Compute TF-IDF weights for the corpus\n",
    "\n",
    "# # Optional: start from zero and fit on tweets\n",
    "# # Initialize TF-IDF Vectorizer\n",
    "# vectorizer_all_tweets = TfidfVectorizer(\n",
    "#     max_features=15000,           # Limit vocabulary size to 10,000 terms\n",
    "#     min_df=3,                     # Ignore terms in fewer than 3 documents\n",
    "#     #max_df=0.90,                  # Ignore overly frequent terms\n",
    "#     sublinear_tf=True,            # Apply logarithmic scaling to term frequencies     \n",
    "#     norm='l2',                    # L2 normalization\n",
    "# ) # Adjust max_features if needed\n",
    "# # Fit the TF-IDF vectorizer on minute-level documents\n",
    "# vectorizer_all_tweets.fit(tweet_documents)\n",
    "\n",
    "# # Save the vectorizer\n",
    "# with open(\"tfidf_vectorizer_all_tweets.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(vectorizer_all_tweets, f)\n",
    "\n",
    "# Load pre-computed\n",
    "with open('tfidf_vectorizer_all_tweets.pkl', 'rb') as f:\n",
    "   vectorizer_all_tweets = pickle.load(f)\n",
    "\n",
    "# Extract TF-IDF weights\n",
    "tfidf_weights_all_tweets = dict(zip(vectorizer_all_tweets.get_feature_names_out(), vectorizer_all_tweets.idf_))\n",
    "\n",
    "\n",
    "# Weighted average embeddings\n",
    "def get_weighted_avg_embedding(tweet, model, vector_size=200, weights=tfidf_weights_all_tweets):\n",
    "    words = tweet.split()\n",
    "    word_vectors = [model[word] * weights.get(word, 1) for word in words if word in model]\n",
    "    if not word_vectors:\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean(word_vectors, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate embeddings for each tweet\n",
    "# # vector_size = 200  # GloVe embedding dimension\n",
    "# tweet_vectors = loaded_df['Tweet'].swifter.apply(lambda tweet: get_weighted_avg_embedding(tweet, model=glove_model, vector_size=200, weights=tfidf_weights_all_tweets))\n",
    "# tweet_vectors = np.array(list(tweet_vectors), dtype=np.float32)\n",
    "\n",
    "# # Save the tweet vectors\n",
    "# with open(\"tweet_vectors_all_data.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(tweet_vectors, f)\n",
    "\n",
    "# print(\"Embeddings saved successfully!\")\n",
    "\n",
    "# Load the tweet vectors\n",
    "with open(\"tweet_vectors_all_data.pkl\", \"rb\") as f:\n",
    "    loaded_tweet_vectors = pickle.load(f)\n",
    "\n",
    "print(\"Embeddings loaded successfully!\")\n",
    "print(\"Loaded vectors shape:\", loaded_tweet_vectors.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Use if no period features ######\n",
    "tweet_df = pd.DataFrame(loaded_tweet_vectors)\n",
    "\n",
    "# Attach the vectors into the original dataframe\n",
    "period_features = pd.concat([loaded_df, tweet_df], axis=1)\n",
    "\n",
    "# Drop the columns that are not useful anymore\n",
    "period_features = period_features.drop(columns=['Timestamp', 'Tweet', 'HomeTeam', 'AwayTeam', 'HomeTeamCode', 'AwayTeamCode'])\n",
    "\n",
    "print(\"X_train_reshaped shape:\", period_features.shape)\n",
    "# Group the tweets into their corresponding periods. This way we generate an average embedding vector for each period\n",
    "period_features = period_features.groupby(['MatchID', 'PeriodID', 'ID']).mean().reset_index()\n",
    "\n",
    "# Save the tweet vectors\n",
    "with open(\"period_features.pkl\", \"wb\") as f:\n",
    "    pickle.dump(period_features, f)\n",
    "\n",
    "print(\"Period features saved successfully!\")\n",
    "\n",
    "# Load the tweet vectors\n",
    "with open(\"period_features.pkl\", \"rb\") as f:\n",
    "    loaded_period_features = pickle.load(f)\n",
    "\n",
    "print(\"Period features loaded successfully!\")\n",
    "print(\"Loaded vectors shape:\", loaded_period_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We drop the non-numerical features and keep the embeddings values for each period\n",
    "X = loaded_period_features.drop(columns=['EventType', 'MatchID', 'ID']).values\n",
    "# We extract the labels of our training samples\n",
    "y = loaded_period_features['EventType'].values\n",
    "\n",
    "\n",
    "# One-hot encode labels\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_encoded = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=SEED)\n",
    "\n",
    "# Add a time step dimension to match the LSTM input shape\n",
    "X_train_reshaped = X_train[:, None, :]  # Add a new axis for timesteps\n",
    "X_test_reshaped = X_test[:, None, :]    # Add a new axis for timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the early stopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',      # Monitor validation loss\n",
    "    patience=5,              # Stop training if no improvement after 3 epochs\n",
    "    restore_best_weights=True  # Restore the best weights when stopping\n",
    ")\n",
    "\n",
    "# Define the LSTM model with deterministic initializers\n",
    "model = Sequential([\n",
    "    tf.keras.layers.Input(shape=(1, X_train_reshaped.shape[2])),  \n",
    "    LSTM(\n",
    "        128, \n",
    "        return_sequences=False, \n",
    "        kernel_initializer=GlorotUniform(seed=SEED), \n",
    "        recurrent_initializer=Orthogonal(seed=SEED),\n",
    "        bias_initializer='zeros'\n",
    "    ),             \n",
    "    Dense(y_encoded.shape[1], activation='softmax', kernel_initializer=GlorotUniform(seed=SEED))\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_reshaped, y_train,\n",
    "                    epochs=50,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[early_stopping],  # Include the early stopping callback\n",
    "                    verbose=1)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test_reshaped, y_test, verbose=1)\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### For Kaggle submission\n",
    "\n",
    "predictions = []\n",
    "dummy_predictions = []\n",
    "# We read each file separately, we preprocess the tweets and then use the classifier to predict the labels.\n",
    "# Finally, we concatenate all predictions into a list that will eventually be concatenated and exported\n",
    "# to be submitted on Kaggle.\n",
    "for fname in sorted(os.listdir(\"eval_tweets\")):\n",
    "    val_df = pd.read_csv(\"eval_tweets/\" + fname)\n",
    "    \n",
    "    home_team, away_team = extract_teams_from_filename(fname)\n",
    "    val_df['HomeTeam'] = home_team\n",
    "    val_df['AwayTeam'] = away_team\n",
    "    \n",
    "    val_df['Tweet'] = val_df.swifter.apply(lambda row: preprocess_text(row['Tweet'], row['HomeTeam'], row['AwayTeam']), axis=1)\n",
    "\n",
    "    # Feature creation\n",
    "    # Add the length of each tweet as a feature\n",
    "    val_df['TweetLength'] = val_df['Tweet'].apply(len)\n",
    "    \n",
    "    # Add a simple tweet count feature\n",
    "    val_df['TweetCount'] = val_df.groupby(['MatchID', 'PeriodID', 'Timestamp'])['Timestamp'].transform('count')\n",
    "    \n",
    "    # Add word count as a feature\n",
    "    val_df['WordCount'] = val_df['Tweet'].apply(lambda x: len(x.split()))\n",
    "\n",
    "    tweet_vectors = val_df['Tweet'].swifter.apply(lambda tweet: get_weighted_avg_embedding(tweet, model=glove_model, vector_size=200, weights= tfidf_weights_all_tweets))\n",
    "\n",
    "    tweet_vectors = np.array(list(tweet_vectors), dtype=np.float32)\n",
    "\n",
    "    tweet_df = pd.DataFrame(tweet_vectors)\n",
    "\n",
    "    period_features_val = pd.concat([val_df, tweet_df], axis=1)\n",
    "    period_features_val = period_features_val.drop(columns=['Timestamp', 'Tweet', 'HomeTeam', 'AwayTeam'])\n",
    "    period_features_val = period_features_val.groupby(['MatchID', 'PeriodID', 'ID']).mean().reset_index()\n",
    "\n",
    "    X = period_features_val.drop(columns=['MatchID', 'ID']).values\n",
    "\n",
    "    # Reshape input for LSTM\n",
    "    X_reshaped = X[:, None, :]  # Add timestep dimension\n",
    "\n",
    "    preds = model.predict(X_reshaped)\n",
    "    preds = preds.argmax(axis=1)  # Convert probabilities to class indices\n",
    "    period_features_val['EventType'] = preds\n",
    "    predictions.append(period_features_val[['ID', 'EventType']])\n",
    "\n",
    "\n",
    "pred_df = pd.concat(predictions)\n",
    "pred_df.to_csv('LSTM_predictions.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Save TF-IDF trained on the training data\n",
    "with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "loaded_df = pd.read_pickle('preprocessed_train.pkl')\n",
    "\n",
    "# Single vectorizer for consistent vocabulary\n",
    "vectorizer = TfidfVectorizer(max_features=20000, stop_words='english', ngram_range=(1, 2))\n",
    "X = vectorizer.fit_transform(loaded_df['Tweet'])\n",
    "\n",
    "# Separate event and non-event tweets\n",
    "event_X = X[loaded_df['EventType'] == 1]\n",
    "non_event_X = X[loaded_df['EventType'] == 0]\n",
    "\n",
    "# Compute mean TF-IDF scores for sparse matrices\n",
    "event_tfidf_scores = np.array(event_X.mean(axis=0)).flatten()\n",
    "non_event_tfidf_scores = np.array(non_event_X.mean(axis=0)).flatten()\n",
    "\n",
    "# Get feature names\n",
    "tfidf_words = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Compute differences and sort\n",
    "tfidf_differences = event_tfidf_scores - non_event_tfidf_scores\n",
    "tfidf_word_differences = sorted(zip(tfidf_words, tfidf_differences), key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "# Output top words\n",
    "print(\"Top words by TF-IDF difference:\")\n",
    "print(tfidf_word_differences[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import chi2\n",
    "import pandas as pd\n",
    "\n",
    "# Convert tweets into a bag-of-words representation\n",
    "vectorizer = CountVectorizer(max_features=5000, stop_words='english', binary=True)\n",
    "X = vectorizer.fit_transform(loaded_df['Tweet'])  # Word presence (binary matrix)\n",
    "y = loaded_df['EventType']  # Binary labels: 1 (event), 0 (no event)\n",
    "\n",
    "# Perform chi-square test\n",
    "chi2_scores, p_values = chi2(X, y)\n",
    "\n",
    "# Create a DataFrame of words and chi-square scores\n",
    "chi2_results = pd.DataFrame({\n",
    "    'Word': vectorizer.get_feature_names_out(),\n",
    "    'Chi2_Score': chi2_scores,\n",
    "    'P_Value': p_values\n",
    "}).sort_values(by='Chi2_Score', ascending=False)\n",
    "\n",
    "# Display the top 10 words most associated with events\n",
    "print(\"Top event-specific words by Chi2 score:\")\n",
    "print(chi2_results.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list = [\n",
    "    'messi', 'ronaldo', 'neymar', 'beckham', 'villa', 'modric', 'suarez', 'griezmann',\n",
    "    'mbappe', 'kroos', 'kane', 'iniesta', 'xavi', 'pogba', 'drogba', 'gerard', 'hummels',\n",
    "    'ribery', 'salah', 'hazard', 'benzema', 'aguero', 'zlatan', 'kaka', 'rooney', 'degea',\n",
    "    'ozil', 'bale', 'schweinsteiger', 'pirlo', 'terry', 'alves', 'pique', 'david', 'cristiano', \n",
    "    'kramer', 'klose', 'muller', 'lahm', 'neuer', 'gotze', 'reus', 'schurrle', 'kroos', 'boateng',\n",
    "    'cromex', 'chicharito', 'torres', 'leroy', 'hernandez', 'christoph', 'vertonghen', 'john',\n",
    "    'javier', 'fernandez', 'slimani', 'matshummels', 'romero', 'porgha', 'marquez', 'guajevilla',\n",
    "    'honsui', 'cahill', 'neymarjr', 'porgha'\n",
    "]\n",
    "\n",
    "\n",
    "def remove_rows_with_names(df, name_list):\n",
    "    # Filter the DataFrame to exclude rows where the \"Word\" column matches any name in the name list\n",
    "    return df[~df['Word'].str.lower().isin(name_list)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_results = remove_rows_with_names(chi2_results, name_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chi2_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = chi2_results[:62]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for the chi² scores\n",
    "chi2_dict = dict(zip(top_words['Word'], top_words['Chi2_Score']))\n",
    "\n",
    "# Compute weighted score features for each important word\n",
    "for word in chi2_dict.keys():\n",
    "    loaded_df[f'WeightedScore_{word}'] = loaded_df['Tweet'].apply(\n",
    "        lambda tweet: tweet.split().count(word) * chi2_dict.get(word, 0)\n",
    "    )\n",
    "\n",
    "print(\"loaded_df shape:\", loaded_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_df.to_pickle('preprocessed_train_extra_weights.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = loaded_df[['TweetLength', 'WordCount']].corr()\n",
    "print(\"Correlation between TweetLength and WordCount:\")\n",
    "print(correlation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(loaded_df['TweetLength'], loaded_df['WordCount'], alpha=0.5)\n",
    "plt.xlabel('TweetLength')\n",
    "plt.ylabel('WordCount')\n",
    "plt.title('Relationship Between Tweet Length and Word Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average word length in a tweet\n",
    "loaded_df['AvgWordLength'] = loaded_df['TweetLength'] / (loaded_df['WordCount'] + 1)  # Add 1 to avoid division by zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
