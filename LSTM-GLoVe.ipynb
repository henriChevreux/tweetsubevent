{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 17:27:36.453988: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/henrichevreux/opt/anaconda3/envs/inf554/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import gensim.downloader as api\n",
    "import re\n",
    "import swifter\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import GlorotUniform, Orthogonal\n",
    "import random\n",
    "# Enable tqdm for pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# Ensure Reproducibility\n",
    "import random\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "\n",
    "# Python's built-in random\n",
    "random.seed(SEED)\n",
    "\n",
    "# NumPy\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# TensorFlow\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Set Python hash seed\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "\n",
    "# Configure TensorFlow for deterministic operations\n",
    "tf.keras.utils.set_random_seed(SEED)  # Sets all random seeds for the program (Python, NumPy, and TensorFlow)\n",
    "tf.config.experimental.enable_op_determinism()  # Enable deterministic operations in TensorFlow\n",
    "\n",
    "# If using GPU, you might also want to set these:\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    # Force TensorFlow to use deterministic GPU operations\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "    # Limit GPU memory growth\n",
    "    for gpu in tf.config.experimental.list_physical_devices('GPU'):\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # Limit to one GPU if using multiple GPUs\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe model\n",
    "glove_model = api.load(\"glove-twitter-200\")  # 200-dimensional GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the average word vector for a tweet\n",
    "def get_avg_embedding(tweet, model, vector_size=200):\n",
    "    words = tweet.split()  # Tokenize by whitespace\n",
    "    word_vectors = [model[word] for word in words if word in model]\n",
    "    if not word_vectors:  # If no words in the tweet are in the vocabulary, return a zero vector\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Tokenization\n",
    "    words = text.split()\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Initialize stopwords and stemmer\n",
    "ps = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Process a single tweet\n",
    "def preprocess_tweet(tweet):\n",
    "    if pd.isna(tweet):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove Retweets using Regex\n",
    "    if re.match(r'^RT\\s+@\\w+:', str(tweet)):\n",
    "        return \"\"\n",
    "\n",
    "    # Clean the Tweet\n",
    "    tweet = re.sub(r\"http\\S+|@\\S+|#\\S+|[^a-zA-Z\\s]\", \"\", str(tweet).lower())\n",
    "    words = tweet.split()\n",
    "    words = [ps.stem(word) for word in words if word not in stop_words]\n",
    "    return \" \".join(words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 5056050/5056050 [03:59<00:00, 21118.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates removed: 523172\n",
      "<bound method NDFrame.head of              ID  MatchID  PeriodID  EventType      Timestamp  \\\n",
      "1           2_0        2         0          0  1403538600000   \n",
      "6           2_0        2         0          0  1403538600000   \n",
      "8           2_0        2         0          0  1403538601000   \n",
      "10          2_0        2         0          0  1403538601000   \n",
      "11          2_0        2         0          0  1403538601000   \n",
      "...         ...      ...       ...        ...            ...   \n",
      "5056038  17_129       17       129          1  1403805600000   \n",
      "5056039  17_129       17       129          1  1403805600000   \n",
      "5056040  17_129       17       129          1  1403805600000   \n",
      "5056042  17_129       17       129          1  1403805600000   \n",
      "5056049  17_129       17       129          1  1403805600000   \n",
      "\n",
      "                                                     Tweet  \n",
      "1        visit offici web site spani real state tourim ...  \n",
      "6                                         today match good  \n",
      "8                                           min go predict  \n",
      "10                        shirt day day get shirt way sinc  \n",
      "11                       squar today last groupwho support  \n",
      "...                                                    ...  \n",
      "5056038                        proud went hate portug love  \n",
      "5056039           win suffici move thru thank take pressur  \n",
      "5056040                    wow awar portug goal differenti  \n",
      "5056042                         portug gone goodby ronaldo  \n",
      "5056049  grew game game one alway proud portug matter f...  \n",
      "\n",
      "[1907145 rows x 6 columns]>\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "folder_path = \"train_tweets\"\n",
    "csv_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(\".csv\")] # Only use first file for testing purposes\n",
    "df = pd.concat((pd.read_csv(f) for f in csv_files), ignore_index=True)\n",
    "\n",
    "# Apply preprocessing\n",
    "df['Tweet'] = df['Tweet'].swifter.apply(preprocess_tweet)\n",
    "# Remove rows where the 'Tweet' column is an empty string\n",
    "df = df[df['Tweet'].str.strip() != \"\"]\n",
    "\n",
    "\n",
    "# Count and remove duplicates\n",
    "before_dedup = len(df)\n",
    "df = df.drop_duplicates(subset='Tweet')\n",
    "after_dedup = len(df)\n",
    "\n",
    "# Display number of duplicates removed\n",
    "num_duplicates_removed = before_dedup - after_dedup\n",
    "print(f\"Number of duplicates removed: {num_duplicates_removed}\")\n",
    "\n",
    "# Reset the index\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             ID  MatchID  PeriodID  EventType      Timestamp  \\\n",
      "0           2_0        2         0          0  1403538600000   \n",
      "1           2_0        2         0          0  1403538600000   \n",
      "2           2_0        2         0          0  1403538601000   \n",
      "3           2_0        2         0          0  1403538601000   \n",
      "4           2_0        2         0          0  1403538601000   \n",
      "...         ...      ...       ...        ...            ...   \n",
      "1907140  17_129       17       129          1  1403805600000   \n",
      "1907141  17_129       17       129          1  1403805600000   \n",
      "1907142  17_129       17       129          1  1403805600000   \n",
      "1907143  17_129       17       129          1  1403805600000   \n",
      "1907144  17_129       17       129          1  1403805600000   \n",
      "\n",
      "                                                     Tweet  \n",
      "0        visit offici web site spani real state tourim ...  \n",
      "1                                         today match good  \n",
      "2                                           min go predict  \n",
      "3                         shirt day day get shirt way sinc  \n",
      "4                        squar today last groupwho support  \n",
      "...                                                    ...  \n",
      "1907140                        proud went hate portug love  \n",
      "1907141           win suffici move thru thank take pressur  \n",
      "1907142                    wow awar portug goal differenti  \n",
      "1907143                         portug gone goodby ronaldo  \n",
      "1907144  grew game game one alway proud portug matter f...  \n",
      "\n",
      "[1907145 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             ID  MatchID  PeriodID  EventType      Timestamp  \\\n",
      "0           2_0        2         0          0  1403538600000   \n",
      "1           2_0        2         0          0  1403538600000   \n",
      "2           2_0        2         0          0  1403538601000   \n",
      "3           2_0        2         0          0  1403538601000   \n",
      "4           2_0        2         0          0  1403538601000   \n",
      "...         ...      ...       ...        ...            ...   \n",
      "1907140  17_129       17       129          1  1403805600000   \n",
      "1907141  17_129       17       129          1  1403805600000   \n",
      "1907142  17_129       17       129          1  1403805600000   \n",
      "1907143  17_129       17       129          1  1403805600000   \n",
      "1907144  17_129       17       129          1  1403805600000   \n",
      "\n",
      "                                                     Tweet  TweetLength  \\\n",
      "0        visit offici web site spani real state tourim ...           53   \n",
      "1                                         today match good           16   \n",
      "2                                           min go predict           14   \n",
      "3                         shirt day day get shirt way sinc           32   \n",
      "4                        squar today last groupwho support           33   \n",
      "...                                                    ...          ...   \n",
      "1907140                        proud went hate portug love           27   \n",
      "1907141           win suffici move thru thank take pressur           40   \n",
      "1907142                    wow awar portug goal differenti           31   \n",
      "1907143                         portug gone goodby ronaldo           26   \n",
      "1907144  grew game game one alway proud portug matter f...           60   \n",
      "\n",
      "         TweetCount  WordCount  \n",
      "0                 2          9  \n",
      "1                 2          3  \n",
      "2                 3          3  \n",
      "3                 3          7  \n",
      "4                 3          5  \n",
      "...             ...        ...  \n",
      "1907140          19          5  \n",
      "1907141          19          7  \n",
      "1907142          19          5  \n",
      "1907143          19          4  \n",
      "1907144          19         11  \n",
      "\n",
      "[1907145 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "# Feature creation\n",
    "# Add the length of each tweet as a feature\n",
    "df['TweetLength'] = df['Tweet'].apply(len)\n",
    "\n",
    "# Add a simple tweet count feature\n",
    "df['TweetCount'] = df.groupby(['MatchID', 'PeriodID', 'Timestamp'])['Timestamp'].transform('count')\n",
    "\n",
    "# Add word count as a feature\n",
    "df['WordCount'] = df['Tweet'].apply(lambda x: len(x.split()))\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "\n",
    "# Compute TF-IDF weights for the corpus\n",
    "\n",
    "# Optional: start from zero and fit on tweets\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "vectorizer.fit(df['Tweet'])\n",
    "\n",
    "\n",
    "# Load pre-computed\n",
    "#with open('tfidf_vectorizer.pkl', 'rb') as f:\n",
    "#    vectorizer = pickle.load(f)\n",
    "\n",
    "\n",
    "tfidf_weights = dict(zip(vectorizer.get_feature_names_out(), vectorizer.idf_))\n",
    "\n",
    "# Weighted average embeddings\n",
    "def get_weighted_avg_embedding(tweet, model, vector_size=200, weights=tfidf_weights):\n",
    "    words = tweet.split()\n",
    "    word_vectors = [model[word] * weights.get(word, 1) for word in words if word in model]\n",
    "    if not word_vectors:\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean(word_vectors, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 1907145/1907145 [01:57<00:00, 16215.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for each tweet\n",
    "vector_size = 200  # GloVe embedding dimension\n",
    "tweet_vectors = df['Tweet'].swifter.apply(lambda tweet: get_weighted_avg_embedding(tweet, model=glove_model, vector_size=200, weights=tfidf_weights))\n",
    "tweet_vectors = np.array(list(tweet_vectors), dtype=np.float32)\n",
    "\n",
    "# Save the tweet vectors\n",
    "# with open(\"tweet_vectors.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(tweet_vectors, f)\n",
    "# \n",
    "# print(\"Embeddings saved successfully!\")\n",
    "# \n",
    "# # Load the tweet vectors\n",
    "# with open(\"tweet_vectors.pkl\", \"rb\") as f:\n",
    "#     loaded_tweet_vectors = pickle.load(f)\n",
    "# \n",
    "# print(\"Embeddings loaded successfully!\")\n",
    "# print(\"Loaded vectors shape:\", loaded_tweet_vectors.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_reshaped shape: (1907145, 207)\n"
     ]
    }
   ],
   "source": [
    "###### Use if no period features ######\n",
    "tweet_df = pd.DataFrame(tweet_vectors)\n",
    "\n",
    "# Attach the vectors into the original dataframe\n",
    "period_features = pd.concat([df, tweet_df], axis=1)\n",
    "\n",
    "# Drop the columns that are not useful anymore\n",
    "period_features = period_features.drop(columns=['Timestamp', 'Tweet'])\n",
    "\n",
    "print(\"X_train_reshaped shape:\", period_features.shape)\n",
    "# Group the tweets into their corresponding periods. This way we generate an average embedding vector for each period\n",
    "period_features = period_features.groupby(['MatchID', 'PeriodID', 'ID']).mean().reset_index()\n",
    "\n",
    "## Save the tweet vectors\n",
    "#with open(\"period_features.pkl\", \"wb\") as f:\n",
    "#    pickle.dump(period_features, f)\n",
    "#\n",
    "#print(\"Period features saved successfully!\")\n",
    "#\n",
    "## Load the tweet vectors\n",
    "#with open(\"period_features_sentiment.pkl\", \"rb\") as f:\n",
    "#    loaded_period_features = pickle.load(f)\n",
    "#\n",
    "#print(\"Period features loaded successfully!\")\n",
    "#print(\"Loaded vectors shape:\", loaded_period_features.shape)\n",
    "#print(loaded_period_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      MatchID  PeriodID      ID  EventType  TweetLength  TweetCount  \\\n",
      "0           0         0     0_0        0.0    44.237288    2.186441   \n",
      "1           0         1     0_1        0.0    43.120000    1.760000   \n",
      "2           0         2     0_2        0.0    43.476190    2.111111   \n",
      "3           0         3     0_3        0.0    39.743243    2.405405   \n",
      "4           0         4     0_4        0.0    41.413793    2.632184   \n",
      "...       ...       ...     ...        ...          ...         ...   \n",
      "2132       19       125  19_125        1.0    32.786948    9.525912   \n",
      "2133       19       126  19_126        1.0    33.613497    8.963190   \n",
      "2134       19       127  19_127        1.0    34.648230    8.690265   \n",
      "2135       19       128  19_128        1.0    34.695876    7.680412   \n",
      "2136       19       129  19_129        1.0    35.677054    6.603399   \n",
      "\n",
      "      WordCount         0         1         2  ...       190       191  \\\n",
      "0      7.559322  0.575533  1.259582  0.292536  ...  0.510598 -0.335112   \n",
      "1      7.480000  0.622914  1.062308  0.457285  ...  0.241044 -0.108536   \n",
      "2      7.539683  0.344310  1.134383  0.314880  ...  0.080313 -0.007642   \n",
      "3      7.013514  0.517660  1.092635  0.154459  ...  0.109117 -0.300220   \n",
      "4      7.126437  0.763914  1.071294  0.065735  ...  0.206904 -0.387670   \n",
      "...         ...       ...       ...       ...  ...       ...       ...   \n",
      "2132   5.865643  0.119268  0.784247  0.358407  ... -0.773587 -0.008076   \n",
      "2133   5.997955  0.088399  0.779074  0.327228  ... -0.727380 -0.092192   \n",
      "2134   6.203540  0.100616  0.807615  0.346193  ... -0.624987 -0.015286   \n",
      "2135   6.288660  0.014446  0.829065  0.278452  ... -0.645727 -0.119599   \n",
      "2136   6.373938  0.104714  0.852306  0.335411  ... -0.666913  0.028533   \n",
      "\n",
      "           192       193       194       195       196       197       198  \\\n",
      "0     0.338040 -0.391013 -0.284575 -0.071377  0.954072  0.564295  0.259750   \n",
      "1     0.159442 -0.584523 -0.126703  0.097590  1.029251  0.493139  0.113539   \n",
      "2     0.354414 -0.398304 -0.059532 -0.153994  0.951995  0.163130  0.360179   \n",
      "3     0.433406 -0.335870  0.111746  0.046487  0.959612  0.386756  0.203093   \n",
      "4     0.246177 -0.478101 -0.165778  0.244525  0.816136  0.611777  0.265646   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "2132  0.436754 -0.169078  0.392479 -0.285390  1.068598  0.038689  0.088742   \n",
      "2133  0.557263 -0.110506  0.358264 -0.292228  1.029684  0.211181  0.171871   \n",
      "2134  0.536936 -0.189609  0.303636 -0.303544  1.113760  0.107775  0.091030   \n",
      "2135  0.578331 -0.230005  0.352509 -0.180217  1.080801  0.212620  0.124951   \n",
      "2136  0.413377 -0.102568  0.359730 -0.285654  1.066336  0.160113  0.043253   \n",
      "\n",
      "           199  \n",
      "0     0.514865  \n",
      "1     0.512230  \n",
      "2     0.303498  \n",
      "3     0.280595  \n",
      "4     0.369368  \n",
      "...        ...  \n",
      "2132  0.210596  \n",
      "2133  0.228469  \n",
      "2134  0.312181  \n",
      "2135  0.281752  \n",
      "2136  0.370615  \n",
      "\n",
      "[2137 rows x 207 columns]\n"
     ]
    }
   ],
   "source": [
    "# We drop the non-numerical features and keep the embeddings values for each period\n",
    "X = period_features.drop(columns=['EventType', 'MatchID', 'ID']).values\n",
    "# We extract the labels of our training samples\n",
    "y = period_features['EventType'].values\n",
    "\n",
    "\n",
    "# One-hot encode labels\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_encoded = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=SEED)\n",
    "\n",
    "# Add a time step dimension to match the LSTM input shape\n",
    "X_train_reshaped = X_train[:, None, :]  # Add a new axis for timesteps\n",
    "X_test_reshaped = X_test[:, None, :]    # Add a new axis for timesteps\n",
    "print(period_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.5915 - loss: 0.6752 - val_accuracy: 0.5994 - val_loss: 0.6505\n",
      "Epoch 2/50\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6537 - loss: 0.6235 - val_accuracy: 0.6667 - val_loss: 0.6163\n",
      "Epoch 3/50\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6909 - loss: 0.5903 - val_accuracy: 0.6988 - val_loss: 0.5906\n",
      "Epoch 4/50\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7116 - loss: 0.5625 - val_accuracy: 0.7076 - val_loss: 0.5763\n",
      "Epoch 5/50\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7305 - loss: 0.5418 - val_accuracy: 0.7222 - val_loss: 0.5648\n",
      "Epoch 6/50\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7375 - loss: 0.5259 - val_accuracy: 0.7222 - val_loss: 0.5580\n",
      "Epoch 7/50\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7477 - loss: 0.5139 - val_accuracy: 0.7310 - val_loss: 0.5529\n",
      "Epoch 8/50\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7572 - loss: 0.5035 - val_accuracy: 0.7427 - val_loss: 0.5498\n",
      "Epoch 9/50\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7590 - loss: 0.4947 - val_accuracy: 0.7427 - val_loss: 0.5466\n",
      "Epoch 10/50\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7627 - loss: 0.4865 - val_accuracy: 0.7573 - val_loss: 0.5443\n",
      "Epoch 11/50\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7692 - loss: 0.4791 - val_accuracy: 0.7602 - val_loss: 0.5424\n",
      "Epoch 12/50\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7742 - loss: 0.4720 - val_accuracy: 0.7573 - val_loss: 0.5409\n",
      "Epoch 13/50\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7759 - loss: 0.4653 - val_accuracy: 0.7573 - val_loss: 0.5399\n",
      "Epoch 14/50\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7828 - loss: 0.4587 - val_accuracy: 0.7573 - val_loss: 0.5396\n",
      "Epoch 15/50\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7880 - loss: 0.4522 - val_accuracy: 0.7544 - val_loss: 0.5394\n",
      "Epoch 16/50\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7877 - loss: 0.4457 - val_accuracy: 0.7485 - val_loss: 0.5395\n",
      "Epoch 17/50\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7904 - loss: 0.4395 - val_accuracy: 0.7427 - val_loss: 0.5399\n",
      "Epoch 18/50\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7970 - loss: 0.4336 - val_accuracy: 0.7456 - val_loss: 0.5406\n",
      "Epoch 19/50\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8012 - loss: 0.4279 - val_accuracy: 0.7515 - val_loss: 0.5411\n",
      "Epoch 20/50\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8100 - loss: 0.4224 - val_accuracy: 0.7427 - val_loss: 0.5419\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7607 - loss: 0.4933\n",
      "Test Accuracy: 0.7617\n"
     ]
    }
   ],
   "source": [
    "# Define the early stopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',      # Monitor validation loss\n",
    "    patience=5,              # Stop training if no improvement after 3 epochs\n",
    "    restore_best_weights=True  # Restore the best weights when stopping\n",
    ")\n",
    "\n",
    "# Define the LSTM model with deterministic initializers\n",
    "model = Sequential([\n",
    "    tf.keras.layers.Input(shape=(1, X_train_reshaped.shape[2])),  \n",
    "    LSTM(\n",
    "        128, \n",
    "        return_sequences=False, \n",
    "        kernel_initializer=GlorotUniform(seed=SEED), \n",
    "        recurrent_initializer=Orthogonal(seed=SEED),\n",
    "        bias_initializer='zeros'\n",
    "    ),             \n",
    "    Dense(y_encoded.shape[1], activation='softmax', kernel_initializer=GlorotUniform(seed=SEED))\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_reshaped, y_train,\n",
    "                    epochs=50,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[early_stopping],  # Include the early stopping callback\n",
    "                    verbose=1)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test_reshaped, y_test, verbose=1)\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 285804/285804 [00:09<00:00, 29294.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates removed: 23919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 89198/89198 [00:04<00:00, 18434.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 45024/45024 [00:04<00:00, 10146.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates removed: 3220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 35175/35175 [00:02<00:00, 12261.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 113402/113402 [00:07<00:00, 15826.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates removed: 12524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 56945/56945 [00:02<00:00, 21371.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 628698/628698 [00:30<00:00, 20441.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates removed: 55427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 260223/260223 [00:13<00:00, 18714.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n"
     ]
    }
   ],
   "source": [
    "###### For Kaggle submission\n",
    "\n",
    "predictions = []\n",
    "dummy_predictions = []\n",
    "# We read each file separately, we preprocess the tweets and then use the classifier to predict the labels.\n",
    "# Finally, we concatenate all predictions into a list that will eventually be concatenated and exported\n",
    "# to be submitted on Kaggle.\n",
    "for fname in sorted(os.listdir(\"eval_tweets\")):\n",
    "    val_df = pd.read_csv(\"eval_tweets/\" + fname)\n",
    "    \n",
    "    val_df['Tweet'] = val_df['Tweet'].swifter.apply(preprocess_tweet)\n",
    "    # Remove rows where the 'Tweet' column is an empty string\n",
    "    val_df = val_df[val_df['Tweet'].str.strip() != \"\"]\n",
    "\n",
    "\n",
    "    # Count and remove duplicates\n",
    "    before_dedup_val = len(val_df)\n",
    "    val_df = val_df.drop_duplicates(subset='Tweet')\n",
    "    after_dedup_val = len(val_df)\n",
    "\n",
    "    # Display number of duplicates removed\n",
    "    num_duplicates_removed_val = before_dedup_val - after_dedup_val\n",
    "    print(f\"Number of duplicates removed: {num_duplicates_removed_val}\")\n",
    "\n",
    "    # Reset the index\n",
    "    val_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Feature creation\n",
    "    # Add the length of each tweet as a feature\n",
    "    val_df['TweetLength'] = val_df['Tweet'].apply(len)\n",
    "    \n",
    "    # Add a simple tweet count feature\n",
    "    val_df['TweetCount'] = val_df.groupby(['MatchID', 'PeriodID', 'Timestamp'])['Timestamp'].transform('count')\n",
    "    \n",
    "    # Add word count as a feature\n",
    "    val_df['WordCount'] = val_df['Tweet'].apply(lambda x: len(x.split()))\n",
    "\n",
    "    tweet_vectors = val_df['Tweet'].swifter.apply(lambda tweet: get_weighted_avg_embedding(tweet, model=glove_model, vector_size=200, weights=tfidf_weights))\n",
    "\n",
    "    tweet_vectors = np.array(list(tweet_vectors), dtype=np.float32)\n",
    "\n",
    "    tweet_df = pd.DataFrame(tweet_vectors)\n",
    "\n",
    "    period_features_val = pd.concat([val_df, tweet_df], axis=1)\n",
    "    period_features_val = period_features_val.drop(columns=['Timestamp', 'Tweet'])\n",
    "    period_features_val = period_features_val.groupby(['MatchID', 'PeriodID', 'ID']).mean().reset_index()\n",
    "\n",
    "    X = period_features_val.drop(columns=['MatchID', 'ID']).values\n",
    "\n",
    "    # Reshape input for LSTM\n",
    "    X_reshaped = X[:, None, :]  # Add timestep dimension\n",
    "\n",
    "    preds = model.predict(X_reshaped)\n",
    "    preds = preds.argmax(axis=1)  # Convert probabilities to class indices\n",
    "    period_features_val['EventType'] = preds\n",
    "    predictions.append(period_features_val[['ID', 'EventType']])\n",
    "\n",
    "\n",
    "pred_df = pd.concat(predictions)\n",
    "pred_df.to_csv('LSTM_predictions.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Save TF-IDF trained on the training data\n",
    "with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inf554",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
