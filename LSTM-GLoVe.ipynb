{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import gensim.downloader as api\n",
    "import re\n",
    "import swifter\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import GlorotUniform, Orthogonal\n",
    "import random\n",
    "# Enable tqdm for pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# Ensure Reproducibility\n",
    "import random\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "\n",
    "# Python's built-in random\n",
    "random.seed(SEED)\n",
    "\n",
    "# NumPy\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# TensorFlow\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Set Python hash seed\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "\n",
    "# Configure TensorFlow for deterministic operations\n",
    "tf.keras.utils.set_random_seed(SEED)  # Sets all random seeds for the program (Python, NumPy, and TensorFlow)\n",
    "tf.config.experimental.enable_op_determinism()  # Enable deterministic operations in TensorFlow\n",
    "\n",
    "# If using GPU, you might also want to set these:\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    # Force TensorFlow to use deterministic GPU operations\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "    # Limit GPU memory growth\n",
    "    for gpu in tf.config.experimental.list_physical_devices('GPU'):\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # Limit to one GPU if using multiple GPUs\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe model\n",
    "glove_model = api.load(\"glove-twitter-200\")  # 200-dimensional GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the average word vector for a tweet\n",
    "def get_avg_embedding(tweet, model, vector_size=200):\n",
    "    words = tweet.split()  # Tokenize by whitespace\n",
    "    word_vectors = [model[word] for word in words if word in model]\n",
    "    if not word_vectors:  # If no words in the tweet are in the vocabulary, return a zero vector\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Tokenization\n",
    "    words = text.split()\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 86843/86843 [00:16<00:00, 5303.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of           ID  MatchID  PeriodID  EventType      Timestamp  \\\n",
      "0        2_0        2         0          0  1403538600000   \n",
      "1        2_0        2         0          0  1403538600000   \n",
      "2        2_0        2         0          0  1403538600000   \n",
      "3        2_0        2         0          0  1403538600000   \n",
      "4        2_0        2         0          0  1403538600000   \n",
      "...      ...      ...       ...        ...            ...   \n",
      "86838  2_129        2       129          1  1403546400000   \n",
      "86839  2_129        2       129          1  1403546400000   \n",
      "86840  2_129        2       129          1  1403546400000   \n",
      "86841  2_129        2       129          1  1403546400000   \n",
      "86842  2_129        2       129          1  1403546400000   \n",
      "\n",
      "                                                   Tweet  \n",
      "0      rt soccerdotcom esp beat au well give away spa...  \n",
      "1      visit sitep official web site httptcoehzkslan ...  \n",
      "2      rt soccerdotcom esp beat au well give away spa...  \n",
      "3      rt worldsoccershop winner au v esp match well ...  \n",
      "4      rt soccerdotcom au beat esp well give away aus...  \n",
      "...                                                  ...  \n",
      "86838  rt soccerdotcom final au esp spain end worldcu...  \n",
      "86839  rt fifaworldcup ft au esp guajevilla torres ju...  \n",
      "86840  end road au esp catching early flight back hom...  \n",
      "86841  rt footballfact worldcup group b final standin...  \n",
      "86842  rt easportsfifa ned top group b perfect record...  \n",
      "\n",
      "[86843 rows x 6 columns]>\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "folder_path = \"train_tweets\"\n",
    "csv_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(\".csv\")][:1] # Only use first file for testing purposes\n",
    "df = pd.concat((pd.read_csv(f) for f in csv_files), ignore_index=True)\n",
    "\n",
    "# Apply preprocessing\n",
    "df['Tweet'] = df['Tweet'].swifter.apply(preprocess_text)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ID  MatchID  PeriodID  EventType      Timestamp  \\\n",
      "0        2_0        2         0          0  1403538600000   \n",
      "1        2_0        2         0          0  1403538600000   \n",
      "2        2_0        2         0          0  1403538600000   \n",
      "3        2_0        2         0          0  1403538600000   \n",
      "4        2_0        2         0          0  1403538600000   \n",
      "...      ...      ...       ...        ...            ...   \n",
      "86838  2_129        2       129          1  1403546400000   \n",
      "86839  2_129        2       129          1  1403546400000   \n",
      "86840  2_129        2       129          1  1403546400000   \n",
      "86841  2_129        2       129          1  1403546400000   \n",
      "86842  2_129        2       129          1  1403546400000   \n",
      "\n",
      "                                                   Tweet  \n",
      "0      rt soccerdotcom esp beat au well give away spa...  \n",
      "1      visit sitep official web site httptcoehzkslan ...  \n",
      "2      rt soccerdotcom esp beat au well give away spa...  \n",
      "3      rt worldsoccershop winner au v esp match well ...  \n",
      "4      rt soccerdotcom au beat esp well give away aus...  \n",
      "...                                                  ...  \n",
      "86838  rt soccerdotcom final au esp spain end worldcu...  \n",
      "86839  rt fifaworldcup ft au esp guajevilla torres ju...  \n",
      "86840  end road au esp catching early flight back hom...  \n",
      "86841  rt footballfact worldcup group b final standin...  \n",
      "86842  rt easportsfifa ned top group b perfect record...  \n",
      "\n",
      "[86843 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             ID  MatchID  PeriodID  EventType      Timestamp  \\\n",
      "0           2_0        2         0          0  1403538600000   \n",
      "1           2_0        2         0          0  1403538600000   \n",
      "2           2_0        2         0          0  1403538600000   \n",
      "3           2_0        2         0          0  1403538600000   \n",
      "4           2_0        2         0          0  1403538600000   \n",
      "...         ...      ...       ...        ...            ...   \n",
      "5056045  17_129       17       129          1  1403805600000   \n",
      "5056046  17_129       17       129          1  1403805600000   \n",
      "5056047  17_129       17       129          1  1403805600000   \n",
      "5056048  17_129       17       129          1  1403805600000   \n",
      "5056049  17_129       17       129          1  1403805600000   \n",
      "\n",
      "                                                     Tweet  TweetLength  \\\n",
      "0        rt soccerdotcom esp beat au well give away spa...          104   \n",
      "1        visit sitep official web site httptcoehzkslan ...           95   \n",
      "2        rt soccerdotcom esp beat au well give away spa...          104   \n",
      "3        rt worldsoccershop winner au v esp match well ...           89   \n",
      "4        rt soccerdotcom au beat esp well give away aus...          103   \n",
      "...                                                    ...          ...   \n",
      "5056045  rt bbcsport portugal fourth team top fifa worl...           96   \n",
      "5056046  rt nbcsports usa move germany beat usmnt portu...           97   \n",
      "5056047  ronaldo could easily scored goal tonight finis...           98   \n",
      "5056048  rt theselenatorboy ppl getting mad bc pepe bra...           98   \n",
      "5056049  grew game game one always proud portugal matte...           65   \n",
      "\n",
      "         TweetCount  WordCount  \n",
      "0                 7         17  \n",
      "1                 7         14  \n",
      "2                 7         17  \n",
      "3                 7         16  \n",
      "4                 7         17  \n",
      "...             ...        ...  \n",
      "5056045          74         17  \n",
      "5056046          74         15  \n",
      "5056047          74         13  \n",
      "5056048          74         15  \n",
      "5056049          74         11  \n",
      "\n",
      "[5056050 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "# Feature creation\n",
    "# Add the length of each tweet as a feature\n",
    "df['TweetLength'] = df['Tweet'].apply(len)\n",
    "\n",
    "# Add a simple tweet count feature\n",
    "df['TweetCount'] = df.groupby(['MatchID', 'PeriodID', 'Timestamp'])['Timestamp'].transform('count')\n",
    "\n",
    "# Add word count as a feature\n",
    "df['WordCount'] = df['Tweet'].apply(lambda x: len(x.split()))\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "\n",
    "# Compute TF-IDF weights for the corpus\n",
    "\n",
    "# Optional: start from zero and fit on tweets\n",
    "# vectorizer = TfidfVectorizer(max_features=10000)\n",
    "# vectorizer.fit(df['Tweet'])\n",
    "\n",
    "\n",
    "# Load pre-computed\n",
    "with open('tfidf_vectorizer.pkl', 'rb') as f:\n",
    "    vectorizer = pickle.load(f)\n",
    "\n",
    "\n",
    "tfidf_weights = dict(zip(vectorizer.get_feature_names_out(), vectorizer.idf_))\n",
    "\n",
    "# Weighted average embeddings\n",
    "def get_weighted_avg_embedding(tweet, model, vector_size=200, weights=tfidf_weights):\n",
    "    words = tweet.split()\n",
    "    word_vectors = [model[word] * weights.get(word, 1) for word in words if word in model]\n",
    "    if not word_vectors:\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean(word_vectors, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings loaded successfully!\n",
      "Loaded vectors shape: (5056050, 200)\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for each tweet\n",
    "# vector_size = 200  # GloVe embedding dimension\n",
    "# tweet_vectors = df['Tweet'].swifter.apply(lambda tweet: get_weighted_avg_embedding(tweet, model=glove_model, vector_size=200, weights=tfidf_weights))\n",
    "# tweet_vectors = np.array(list(tweet_vectors), dtype=np.float32)\n",
    "\n",
    "# Save the tweet vectors\n",
    "# with open(\"tweet_vectors.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(tweet_vectors, f)\n",
    "# \n",
    "# print(\"Embeddings saved successfully!\")\n",
    "\n",
    "# Load the tweet vectors\n",
    "with open(\"tweet_vectors.pkl\", \"rb\") as f:\n",
    "    loaded_tweet_vectors = pickle.load(f)\n",
    "\n",
    "print(\"Embeddings loaded successfully!\")\n",
    "print(\"Loaded vectors shape:\", loaded_tweet_vectors.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Period features loaded successfully!\n",
      "Loaded vectors shape: (2137, 207)\n",
      "      MatchID  PeriodID      ID  EventType  TweetLength  TweetCount  \\\n",
      "0           0         0     0_0        0.0    90.829167    5.041667   \n",
      "1           0         1     0_1        0.0    85.770053    4.358289   \n",
      "2           0         2     0_2        0.0    87.784810    4.881857   \n",
      "3           0         3     0_3        0.0    85.276451    6.440273   \n",
      "4           0         4     0_4        0.0    89.645309   13.732265   \n",
      "...       ...       ...     ...        ...          ...         ...   \n",
      "2132       19       125  19_125        1.0    77.492706   76.553492   \n",
      "2133       19       126  19_126        1.0    79.686723   85.419807   \n",
      "2134       19       127  19_127        1.0    79.435999   84.739234   \n",
      "2135       19       128  19_128        1.0    79.569395   75.923932   \n",
      "2136       19       129  19_129        1.0    84.503898   75.678102   \n",
      "\n",
      "      WordCount         0         1         2  ...       190       191  \\\n",
      "0     12.537500  0.488050  1.042977  0.022373  ... -0.125250 -0.179736   \n",
      "1     12.331551  0.582401  1.021592  0.027417  ... -0.059809 -0.144766   \n",
      "2     12.805907  0.520287  1.141662  0.100676  ... -0.176729 -0.169048   \n",
      "3     12.262799  0.569246  0.899488  0.000605  ... -0.247600 -0.081040   \n",
      "4     12.558352  0.471500  0.826557 -0.211539  ... -0.156128 -0.018429   \n",
      "...         ...       ...       ...       ...  ...       ...       ...   \n",
      "2132  12.523210  0.059476  1.053161  0.289900  ... -0.190853 -0.299434   \n",
      "2133  12.464983  0.073222  1.055907  0.323197  ... -0.156078 -0.321968   \n",
      "2134  12.361183  0.088560  1.062610  0.285056  ... -0.170830 -0.310737   \n",
      "2135  12.468639  0.112299  1.043302  0.293926  ... -0.142051 -0.284659   \n",
      "2136  13.698597  0.086424  0.764695  0.534239  ...  0.003196 -0.014678   \n",
      "\n",
      "           192       193       194       195       196       197       198  \\\n",
      "0     0.323705 -0.138897 -0.268496  0.017094  1.134220  0.485171  0.308856   \n",
      "1     0.381236 -0.140269 -0.430301  0.104825  1.059953  0.522621  0.348112   \n",
      "2     0.434229 -0.153739 -0.425276  0.056216  1.121426  0.423600  0.385780   \n",
      "3     0.350632  0.030867 -0.456320  0.038304  1.107222  0.535543  0.479922   \n",
      "4     0.139764  0.035421 -0.421111  0.142205  1.081565  0.608661  0.375504   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "2132 -0.034629  0.011046 -0.021627  0.041810  0.990698  0.531021  0.036651   \n",
      "2133  0.065973 -0.008849 -0.073792  0.099337  0.961236  0.532253  0.008066   \n",
      "2134  0.066793  0.005102 -0.063630  0.099222  0.967656  0.546858 -0.012973   \n",
      "2135  0.028472 -0.005419 -0.112005  0.135496  0.947655  0.510893 -0.076580   \n",
      "2136 -0.066658 -0.032847 -0.532502  0.331458  0.752004  0.348347 -0.316976   \n",
      "\n",
      "           199  \n",
      "0     0.757475  \n",
      "1     0.708246  \n",
      "2     0.721273  \n",
      "3     0.714815  \n",
      "4     0.954719  \n",
      "...        ...  \n",
      "2132  0.663630  \n",
      "2133  0.719238  \n",
      "2134  0.733166  \n",
      "2135  0.728509  \n",
      "2136  0.780119  \n",
      "\n",
      "[2137 rows x 207 columns]\n"
     ]
    }
   ],
   "source": [
    "###### Use if no period features ######\n",
    "# tweet_df = pd.DataFrame(loaded_tweet_vectors)\n",
    "# \n",
    "# # Attach the vectors into the original dataframe\n",
    "# period_features = pd.concat([df, tweet_df], axis=1)\n",
    "# \n",
    "# # Drop the columns that are not useful anymore\n",
    "# period_features = period_features.drop(columns=['Timestamp', 'Tweet'])\n",
    "# \n",
    "# print(\"X_train_reshaped shape:\", period_features.shape)\n",
    "# # Group the tweets into their corresponding periods. This way we generate an average embedding vector for each period\n",
    "# period_features = period_features.groupby(['MatchID', 'PeriodID', 'ID']).mean().reset_index()\n",
    "# \n",
    "# # Save the tweet vectors\n",
    "# with open(\"period_features.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(period_features, f)\n",
    "# \n",
    "# print(\"Period features saved successfully!\")\n",
    "\n",
    "# Load the tweet vectors\n",
    "import pickle\n",
    "with open(\"period_features.pkl\", \"rb\") as f:\n",
    "    loaded_period_features = pickle.load(f)\n",
    "\n",
    "print(\"Period features loaded successfully!\")\n",
    "print(\"Loaded vectors shape:\", loaded_period_features.shape)\n",
    "print(loaded_period_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1709, 204)\n",
      "X_test shape: (428, 204)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Drop non-numerical features\n",
    "X = loaded_period_features.drop(columns=['EventType', 'MatchID', 'ID']).values\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Extract labels\n",
    "y = loaded_period_features['EventType'].values\n",
    "\n",
    "# One-hot encode labels\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_encoded = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=SEED)\n",
    "\n",
    "# Print shapes for verification\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "Best parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'subsample': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'n_estimators': [100, 200],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(\n",
    "    XGBClassifier(objective='binary:logistic', seed=SEED),\n",
    "    param_grid, cv=5, scoring='f1', verbose=1\n",
    ")\n",
    "grid_search.fit(X_train, y_train.argmax(axis=1))\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation Accuracy: 0.7326 ± 0.0238\n",
      "Test Accuracy: 0.7897\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.81      0.79       203\n",
      "           1       0.82      0.77      0.79       225\n",
      "\n",
      "    accuracy                           0.79       428\n",
      "   macro avg       0.79      0.79      0.79       428\n",
      "weighted avg       0.79      0.79      0.79       428\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Calculate scale_pos_weight\n",
    "num_pos = sum(y_train.argmax(axis=1) == 1)\n",
    "num_neg = sum(y_train.argmax(axis=1) == 0)\n",
    "scale_pos_weight = num_neg / num_pos\n",
    "\n",
    "# Initialize XGBoost model with regularization\n",
    "xgb_model = XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    max_depth=5,\n",
    "    learning_rate=0.01,\n",
    "    n_estimators=200,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    reg_alpha=0.5,  # L1 regularization\n",
    "    reg_lambda=1,   # L2 regularization\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(\n",
    "    xgb_model, X_train, y_train.argmax(axis=1), cv=5, scoring='accuracy'\n",
    ")\n",
    "\n",
    "# Print cross-validation results\n",
    "print(f\"Cross-validation Accuracy: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "\n",
    "# Train the model on the entire training set\n",
    "xgb_model.fit(X_train, y_train.argmax(axis=1))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test.argmax(axis=1), y_pred)\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test.argmax(axis=1), y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 285804/285804 [00:47<00:00, 5991.43it/s]\n",
      "Pandas Apply: 100%|██████████| 285804/285804 [00:15<00:00, 18258.13it/s]\n",
      "Pandas Apply: 100%|██████████| 45024/45024 [00:07<00:00, 6169.60it/s]\n",
      "Pandas Apply: 100%|██████████| 45024/45024 [00:02<00:00, 17259.49it/s]\n",
      "Pandas Apply: 100%|██████████| 113402/113402 [00:18<00:00, 6284.89it/s]\n",
      "Pandas Apply: 100%|██████████| 113402/113402 [00:05<00:00, 19685.13it/s]\n",
      "Pandas Apply: 100%|██████████| 628698/628698 [01:39<00:00, 6325.44it/s]\n",
      "Pandas Apply: 100%|██████████| 628698/628698 [00:31<00:00, 20123.59it/s]\n"
     ]
    }
   ],
   "source": [
    "###### For Kaggle submission\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Initialize lists for storing predictions\n",
    "predictions = []\n",
    "\n",
    "# Loop through evaluation tweets files\n",
    "for fname in sorted(os.listdir(\"eval_tweets\")):\n",
    "    val_df = pd.read_csv(f\"eval_tweets/{fname}\")\n",
    "\n",
    "    # Preprocess tweets\n",
    "    val_df['Tweet'] = val_df['Tweet'].swifter.apply(preprocess_text)\n",
    "\n",
    "    # Feature creation\n",
    "    val_df['TweetLength'] = val_df['Tweet'].apply(len)\n",
    "    val_df['TweetCount'] = val_df.groupby(['MatchID', 'PeriodID', 'Timestamp'])['Timestamp'].transform('count')\n",
    "    val_df['WordCount'] = val_df['Tweet'].apply(lambda x: len(x.split()))\n",
    "\n",
    "    # Generate tweet embeddings\n",
    "    tweet_vectors = val_df['Tweet'].swifter.apply(lambda tweet: get_weighted_avg_embedding(tweet, model=glove_model, vector_size=200, weights=tfidf_weights))\n",
    "    tweet_vectors = np.array(list(tweet_vectors), dtype=np.float32)\n",
    "\n",
    "    # Combine embeddings with features\n",
    "    tweet_df = pd.DataFrame(tweet_vectors)\n",
    "    period_features_val = pd.concat([val_df, tweet_df], axis=1)\n",
    "    period_features_val = period_features_val.drop(columns=['Timestamp', 'Tweet'])\n",
    "    period_features_val = period_features_val.groupby(['MatchID', 'PeriodID', 'ID']).mean().reset_index()\n",
    "\n",
    "    # Prepare input features for the model\n",
    "    X = period_features_val.drop(columns=['MatchID', 'ID']).values\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Predict using the XGBoost model\n",
    "    preds = xgb_model.predict(X_scaled)\n",
    "\n",
    "    # Store predictions\n",
    "    period_features_val['EventType'] = preds\n",
    "    predictions.append(period_features_val[['ID', 'EventType']])\n",
    "\n",
    "# Concatenate all predictions and export to CSV\n",
    "pred_df = pd.concat(predictions)\n",
    "pred_df.to_csv('XGBoost_predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inf554",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
