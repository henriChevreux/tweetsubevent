{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-11 10:42:43.599306: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 604, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/zk/yx6v2s9j65vf9txb_j71tvn00000gn/T/ipykernel_17006/782360658.py\", line 4, in <module>\n",
      "    import tensorflow as tf\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/tensorflow/__init__.py\", line 45, in <module>\n",
      "    from tensorflow._api.v2 import __internal__\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/tensorflow/_api/v2/__internal__/__init__.py\", line 8, in <module>\n",
      "    from tensorflow._api.v2.__internal__ import autograph\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/tensorflow/_api/v2/__internal__/autograph/__init__.py\", line 8, in <module>\n",
      "    from tensorflow.python.autograph.core.ag_ctx import control_status_ctx # line: 34\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/tensorflow/python/autograph/core/ag_ctx.py\", line 21, in <module>\n",
      "    from tensorflow.python.autograph.utils import ag_logging\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/tensorflow/python/autograph/utils/__init__.py\", line 17, in <module>\n",
      "    from tensorflow.python.autograph.utils.context_managers import control_dependency_on_returns\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/tensorflow/python/autograph/utils/context_managers.py\", line 19, in <module>\n",
      "    from tensorflow.python.framework import ops\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/tensorflow/python/framework/ops.py\", line 46, in <module>\n",
      "    from tensorflow.python import pywrap_tfe\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/tensorflow/python/pywrap_tfe.py\", line 25, in <module>\n",
      "    from tensorflow.python._pywrap_tfe import *\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 604, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/zk/yx6v2s9j65vf9txb_j71tvn00000gn/T/ipykernel_17006/782360658.py\", line 4, in <module>\n",
      "    import tensorflow as tf\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/tensorflow/__init__.py\", line 45, in <module>\n",
      "    from tensorflow._api.v2 import __internal__\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/tensorflow/_api/v2/__internal__/__init__.py\", line 8, in <module>\n",
      "    from tensorflow._api.v2.__internal__ import autograph\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/tensorflow/_api/v2/__internal__/autograph/__init__.py\", line 8, in <module>\n",
      "    from tensorflow.python.autograph.core.ag_ctx import control_status_ctx # line: 34\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/tensorflow/python/autograph/core/ag_ctx.py\", line 21, in <module>\n",
      "    from tensorflow.python.autograph.utils import ag_logging\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/tensorflow/python/autograph/utils/__init__.py\", line 17, in <module>\n",
      "    from tensorflow.python.autograph.utils.context_managers import control_dependency_on_returns\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/tensorflow/python/autograph/utils/context_managers.py\", line 19, in <module>\n",
      "    from tensorflow.python.framework import ops\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/tensorflow/python/framework/ops.py\", line 49, in <module>\n",
      "    from tensorflow.python.client import pywrap_tf_session\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/tensorflow/python/client/pywrap_tf_session.py\", line 19, in <module>\n",
      "    from tensorflow.python.client._pywrap_tf_session import *\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 604, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/zk/yx6v2s9j65vf9txb_j71tvn00000gn/T/ipykernel_17006/782360658.py\", line 4, in <module>\n",
      "    import tensorflow as tf\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/tensorflow/__init__.py\", line 45, in <module>\n",
      "    from tensorflow._api.v2 import __internal__\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/tensorflow/_api/v2/__internal__/__init__.py\", line 8, in <module>\n",
      "    from tensorflow._api.v2.__internal__ import autograph\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/tensorflow/_api/v2/__internal__/autograph/__init__.py\", line 8, in <module>\n",
      "    from tensorflow.python.autograph.core.ag_ctx import control_status_ctx # line: 34\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/tensorflow/python/autograph/core/ag_ctx.py\", line 21, in <module>\n",
      "    from tensorflow.python.autograph.utils import ag_logging\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/tensorflow/python/autograph/utils/__init__.py\", line 17, in <module>\n",
      "    from tensorflow.python.autograph.utils.context_managers import control_dependency_on_returns\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/tensorflow/python/autograph/utils/context_managers.py\", line 19, in <module>\n",
      "    from tensorflow.python.framework import ops\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/tensorflow/python/framework/ops.py\", line 50, in <module>\n",
      "    from tensorflow.python.eager import context\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/tensorflow/python/eager/context.py\", line 37, in <module>\n",
      "    from tensorflow.python.eager import execute\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/tensorflow/python/eager/execute.py\", line 21, in <module>\n",
      "    from tensorflow.python.framework import dtypes\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/tensorflow/python/framework/dtypes.py\", line 21, in <module>\n",
      "    import ml_dtypes\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/ml_dtypes/__init__.py\", line 32, in <module>\n",
      "    from ml_dtypes._finfo import finfo\n",
      "  File \"/Users/charles/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/ml_dtypes/_finfo.py\", line 19, in <module>\n",
      "    from ml_dtypes._ml_dtypes_ext import bfloat16\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core._multiarray_umath failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;31mImportError\u001b[0m: numpy.core._multiarray_umath failed to import"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.umath failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LSTM, Dense\n",
      "File \u001b[0;32m~/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/tensorflow/__init__.py:45\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2 \u001b[38;5;28;01mas\u001b[39;00m _tf2\n\u001b[1;32m     43\u001b[0m _tf2\u001b[38;5;241m.\u001b[39menable()\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __operators__\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio\n",
      "File \u001b[0;32m~/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/tensorflow/_api/v2/__internal__/__init__.py:8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__ namespace\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m decorator\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dispatch\n",
      "File \u001b[0;32m~/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/tensorflow/_api/v2/__internal__/autograph/__init__.py:8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__.autograph namespace\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mag_ctx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_status_ctx \u001b[38;5;66;03m# line: 34\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_convert \u001b[38;5;66;03m# line: 493\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/tensorflow/python/autograph/core/ag_ctx.py:21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mthreading\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ag_logging\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_export\n\u001b[1;32m     25\u001b[0m stacks \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mlocal()\n",
      "File \u001b[0;32m~/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/tensorflow/python/autograph/utils/__init__.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Utility module that contains APIs usable in the generated code.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext_managers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_dependency_on_returns\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m alias_tensors\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor_list\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dynamic_list_append\n",
      "File \u001b[0;32m~/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/tensorflow/python/autograph/utils/context_managers.py:19\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Various context managers.\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcontextlib\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_array_ops\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontrol_dependency_on_returns\u001b[39m(return_value):\n",
      "File \u001b[0;32m~/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:50\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tf_session\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m monitoring\n",
      "File \u001b[0;32m~/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/tensorflow/python/eager/context.py:37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tf_session\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cancellation\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m execute\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m executor\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m monitoring\n",
      "File \u001b[0;32m~/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tfe\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtypes\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_conversion_registry\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_shape\n",
      "File \u001b[0;32m~/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/tensorflow/python/framework/dtypes.py:21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdataclasses\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Type, Sequence, Optional\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mml_dtypes\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m types_pb2\n",
      "File \u001b[0;32m~/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/ml_dtypes/__init__.py:32\u001b[0m\n\u001b[1;32m     16\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbfloat16\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muint4\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     28\u001b[0m ]\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Type\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mml_dtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_finfo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m finfo\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mml_dtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_iinfo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m iinfo\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mml_dtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ml_dtypes_ext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bfloat16\n",
      "File \u001b[0;32m~/Desktop/STUDY/Polytechnique - MScT AI & VC/Courses/M1/S1/Machine and Deep Learning /END_PROJECT/.venv/lib/python3.11/site-packages/ml_dtypes/_finfo.py:19\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Overload of numpy.finfo to handle dtypes defined in ml_dtypes.\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mml_dtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ml_dtypes_ext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bfloat16\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mml_dtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ml_dtypes_ext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m float8_e4m3b11fnuz\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mml_dtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ml_dtypes_ext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m float8_e4m3fn\n",
      "\u001b[0;31mImportError\u001b[0m: numpy.core.umath failed to import"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import gensim.downloader as api\n",
    "import re\n",
    "import swifter\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import GlorotUniform, Orthogonal\n",
    "import random\n",
    "# Enable tqdm for pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# Ensure Reproducibility\n",
    "import random\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "\n",
    "# Python's built-in random\n",
    "random.seed(SEED)\n",
    "\n",
    "# NumPy\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# TensorFlow\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Set Python hash seed\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "\n",
    "# Configure TensorFlow for deterministic operations\n",
    "tf.keras.utils.set_random_seed(SEED)  # Sets all random seeds for the program (Python, NumPy, and TensorFlow)\n",
    "tf.config.experimental.enable_op_determinism()  # Enable deterministic operations in TensorFlow\n",
    "\n",
    "# If using GPU, you might also want to set these:\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    # Force TensorFlow to use deterministic GPU operations\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "    # Limit GPU memory growth\n",
    "    for gpu in tf.config.experimental.list_physical_devices('GPU'):\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # Limit to one GPU if using multiple GPUs\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe model\n",
    "glove_model = api.load(\"glove-twitter-200\")  # 200-dimensional GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Country list for team extraction\n",
    "country_list = [\n",
    "    \"Argentina\", \"Belgium\", \"Germany\", \"Serbia\", \"Greece\", \"IvoryCoast\", \n",
    "    \"Netherlands\", \"Mexico\", \"Australia\", \"Spain\", \"SouthKorea\", \n",
    "    \"Cameroon\", \"Brazil\", \"France\", \"Nigeria\", \"Algeria\", \"USA\", \n",
    "    \"Honduras\", \"Switzerland\", \"Croatia\", \"Chile\", \"Portugal\", \n",
    "    \"Ghana\", \"Slovenia\"\n",
    "]\n",
    "\n",
    "country_variations = {\n",
    "    'argentina': ['argentina', 'arg', 'argentine', 'argies', 'albiceleste', 'argentinian', 'argentinos', 'argentinas'],\n",
    "    'australia': ['australia', 'aus', 'aussie', 'aussies', 'socceroos', 'oz', 'straya', 'australian', 'au'],\n",
    "    'belgium': ['belgium', 'bel', 'belgique', 'belgie', 'belgian', 'belgians', 'red devils', 'diables rouges'],\n",
    "    'brazil': ['brazil', 'bra', 'brasil', 'bresil', 'brazilian', 'brazilians', 'selecao', 'canarinho', 'verde amarela', 'samba boys'],\n",
    "    'cameroon': ['cameroon', 'cmr', 'cameroun', 'camerounais', 'indomitable lions', 'lions'],\n",
    "    'france': ['france', 'fra', 'french', 'les bleus', 'tricolore', 'tricolores', 'equipe de france', 'allez les bleus'],\n",
    "    'honduras': ['honduras', 'hon', 'honduran', 'hondurans', 'los catrachos', 'catrachos', 'la h'],\n",
    "    'portugal': ['portugal', 'por', 'portuguese', 'selecao das quinas', 'seleccao', 'navegadores', 'team portugal'],\n",
    "    'spain': ['spain', 'esp', 'espana', 'espania', 'spanish', 'la roja', 'furia roja', 'la furia', 'la seleccion'],\n",
    "    'southkorea': ['south korea', 'korea', 'kor', 'skorea', 'korean', 'koreans', 'taeguk warriors', 'warriors'],\n",
    "    'switzerland': ['switzerland', 'sui', 'suisse', 'schweiz', 'swiss', 'nati', 'rossocrociati', 'a team'],\n",
    "    'usa': ['usa', 'united states', 'america', 'united states of america', 'us', 'usa', 'usmnt', 'americans', 'american', 'yanks', 'uncle sam', 'stars and stripes', 'team usa'],\n",
    "    'ghana': ['ghana', 'gha', 'ghanaian', 'ghanaians', 'black stars', 'stars'],\n",
    "    'netherlands': ['netherlands', 'ned', 'holland', 'dutch', 'oranje', 'flying dutchmen', 'orange', 'clockwork orange', 'nederlands'],\n",
    "    'germany': ['germany', 'ger', 'alemania', 'deutschland', 'german', 'germans', 'die mannschaft', 'nationalelf', 'deu'],\n",
    "    'iran': ['iran', 'irn', 'iranian', 'iranians', 'team melli', 'persian stars'],\n",
    "    'nigeria': ['nigeria', 'nga', 'naija', 'super eagles', 'eagles', 'nigerian', 'nigerians', 'green eagles'],\n",
    "    'algeria': ['algeria', 'alg', 'algerian', 'algerians', 'fennecs', 'desert foxes', 'les verts'],\n",
    "    'croatia': ['croatia', 'cro', 'hrvatska', 'hrv', 'croatian', 'croatians', 'vatreni', 'blazers', 'kockasti'],\n",
    "    'chile': ['chile', 'chi', 'chilean', 'chileans', 'la roja', 'team chile'],\n",
    "    'slovenia': ['slovenia', 'svn', 'slovenian', 'slovenians', 'slovenski', 'boys'],\n",
    "    'serbia': ['serbia', 'srb', 'serbian', 'serbians', 'beli orlovi', 'white eagles', 'orlovi'],\n",
    "    'greece': ['greece', 'gre', 'greek', 'greeks', 'piratiko', 'ethniki', 'galanolefki'],\n",
    "    'ivorycoast': ['ivory coast', 'civ', 'cote divoire', 'cotedivoire', 'ivorians', 'les elephants', 'elephants', 'ivory'],\n",
    "    'mexico': ['mexico', 'mex', 'mexiko', 'mexican', 'mexicans', 'el tri', 'tricolor', 'aztecas', 'el tricolor', 'verde']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the average word vector for a tweet\n",
    "def get_avg_embedding(tweet, model, vector_size=200):\n",
    "    words = tweet.split()  # Tokenize by whitespace\n",
    "    word_vectors = [model[word] for word in words if word in model]\n",
    "    if not word_vectors:  # If no words in the tweet are in the vocabulary, return a zero vector\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "def extract_teams_from_filename(filename):\n",
    "    # Remove numbers and file extension\n",
    "    base_name = re.sub(r'\\d+\\.csv$', '', filename)\n",
    "    \n",
    "    # Identify teams from the predefined country list\n",
    "    teams = [country for country in country_list if country in base_name]\n",
    "    \n",
    "    if len(teams) >= 2:\n",
    "        return teams[0], teams[1]\n",
    "    elif len(teams) == 1:\n",
    "        return teams[0], \"Unknown\"\n",
    "    else:\n",
    "        return \"Unknown\", \"Unknown\"\n",
    "    \n",
    "def normalize_countries_with_teams(words, home_team, away_team):\n",
    "    # Text is already preprocessed and split into words\n",
    "    normalized_words = words.copy()\n",
    "    \n",
    "    # Replace individual country mentions\n",
    "    for i, word in enumerate(words):\n",
    "        # Check if it's home team\n",
    "        if word in country_variations.get(home_team.lower(), []):\n",
    "            normalized_words[i] = 'hometeam'\n",
    "        # Check if it's away team\n",
    "        elif word in country_variations.get(away_team.lower(), []):\n",
    "            normalized_words[i] = 'awayteam'\n",
    "        else:\n",
    "            # Check if it's any other country\n",
    "            for country, variations in country_variations.items():\n",
    "                if word in variations and country.lower() not in [home_team.lower(), away_team.lower()]:\n",
    "                    normalized_words[i] = 'othercountry'\n",
    "                    break\n",
    "    \n",
    "    return normalized_words\n",
    "\n",
    "\n",
    "def preprocess_text(text, hometeam, awayteam):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Tokenization\n",
    "    words = text.split()\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    words = [word for word in words if not word.startswith('http')]\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    # Apply country normalization if matchID is provided\n",
    "    words = normalize_countries_with_teams(words, hometeam, awayteam)\n",
    "    \n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "folder_path = \"train_tweets\"\n",
    "csv_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(\".csv\")]\n",
    "# First load all data with team information\n",
    "dataframes = []\n",
    "for file_path in csv_files:\n",
    "    # Load the CSV\n",
    "    current_df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Get filename and extract teams\n",
    "    filename = os.path.basename(file_path)\n",
    "    home_team, away_team = extract_teams_from_filename(filename)\n",
    "    \n",
    "    # Add team columns\n",
    "    current_df['HomeTeam'] = home_team\n",
    "    current_df['AwayTeam'] = away_team\n",
    "    \n",
    "    dataframes.append(current_df)\n",
    "\n",
    "# Concatenate all dataframes\n",
    "df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "\n",
    "# Then apply regular preprocessing with MatchID\n",
    "df['Tweet'] = df.swifter.apply(lambda row: preprocess_text(row['Tweet'], row['HomeTeam'], row['AwayTeam']), axis=1)\n",
    "\n",
    "df.to_pickle('preprocessed_train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature creation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "MMscaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "loaded_df = pd.read_pickle('preprocessed_train.pkl')\n",
    "\n",
    "# Add the length of each tweet as a feature\n",
    "loaded_df['TweetLength'] = loaded_df['Tweet'].apply(len)\n",
    "\n",
    "# Add a simple tweet count feature\n",
    "loaded_df['TweetCount'] = loaded_df.groupby(['MatchID', 'PeriodID', 'Timestamp'])['Timestamp'].transform('count')\n",
    "\n",
    "# Add word count as a feature\n",
    "loaded_df['WordCount'] = loaded_df['Tweet'].apply(lambda x: len(x.split()))\n",
    "\n",
    "loaded_df['TweetLength'] = MMscaler.fit_transform(loaded_df['TweetLength'].values.reshape(-1, 1))\n",
    "# Add 1 to avoid division by zero\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(loaded_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*PREPROCESSING ALL DATA AND SAVING IT*\n",
    "\n",
    "Use this to make weights and tfidf, Could be more efficient. But eitherway nice to have preprocessed df saved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####PREPROCESSING ALL DATA AND SAVING IT#####\n",
    "\n",
    "# # Directories\n",
    "# input_folder = \"all_tweets\"\n",
    "# output_folder = \"all_tweets_preprocessed\"\n",
    "# os.makedirs(output_folder, exist_ok=True)  # Ensure the output folder exists\n",
    "\n",
    "# # Process each file in the input folder\n",
    "# csv_files = [os.path.join(input_folder, f) for f in os.listdir(input_folder) if f.endswith(\".csv\")]\n",
    "# for file_path in csv_files:\n",
    "#     # Load the CSV\n",
    "#     current_df = pd.read_csv(file_path)\n",
    "    \n",
    "#     # Extract teams from filename\n",
    "#     filename = os.path.basename(file_path)\n",
    "#     home_team, away_team = extract_teams_from_filename(filename)  # Use your existing function\n",
    "    \n",
    "#     # Preprocess the tweets\n",
    "#     current_df['Tweet'] = current_df.swifter.apply(\n",
    "#         lambda row: preprocess_text(row['Tweet'], home_team, away_team), axis=1\n",
    "#     )\n",
    "    \n",
    "#     # Save the preprocessed data to the output folder\n",
    "#     output_path = os.path.join(output_folder, filename)\n",
    "#     current_df.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "\n",
    "# # Folder containing preprocessed tweets\n",
    "# input_folder = \"all_tweets_preprocessed\"\n",
    "\n",
    "# # Collect all tweets grouped by minute across all files\n",
    "# tweet_documents = []\n",
    "\n",
    "# # Process each preprocessed file\n",
    "# csv_files = [os.path.join(input_folder, f) for f in os.listdir(input_folder) if f.endswith(\".csv\")]\n",
    "# for file_path in csv_files:\n",
    "#     # Load the preprocessed CSV\n",
    "#     df_all_tweets = pd.read_csv(file_path)\n",
    "#     tweet_documents.extend(df_all_tweets['Tweet'].tolist())\n",
    "    \n",
    "\n",
    "# # Compute TF-IDF weights for the corpus\n",
    "\n",
    "# # Optional: start from zero and fit on tweets\n",
    "# # Initialize TF-IDF Vectorizer\n",
    "# vectorizer_all_tweets = TfidfVectorizer(\n",
    "#     max_features=15000,           # Limit vocabulary size to 10,000 terms\n",
    "#     min_df=3,                     # Ignore terms in fewer than 3 documents\n",
    "#     #max_df=0.90,                  # Ignore overly frequent terms\n",
    "#     sublinear_tf=True,            # Apply logarithmic scaling to term frequencies     \n",
    "#     norm='l2',                    # L2 normalization\n",
    "# ) # Adjust max_features if needed\n",
    "# # Fit the TF-IDF vectorizer on minute-level documents\n",
    "# vectorizer_all_tweets.fit(tweet_documents)\n",
    "\n",
    "# # Save the vectorizer\n",
    "# with open(\"tfidf_vectorizer_all_tweets.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(vectorizer_all_tweets, f)\n",
    "\n",
    "# Load pre-computed\n",
    "with open('tfidf_vectorizer_all_tweets.pkl', 'rb') as f:\n",
    "   vectorizer_all_tweets = pickle.load(f)\n",
    "\n",
    "# Extract TF-IDF weights\n",
    "tfidf_weights_all_tweets = dict(zip(vectorizer_all_tweets.get_feature_names_out(), vectorizer_all_tweets.idf_))\n",
    "\n",
    "\n",
    "# Weighted average embeddings\n",
    "def get_weighted_avg_embedding(tweet, model, vector_size=200, weights=tfidf_weights_all_tweets):\n",
    "    words = tweet.split()\n",
    "    word_vectors = [model[word] * weights.get(word, 1) for word in words if word in model]\n",
    "    if not word_vectors:\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean(word_vectors, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate embeddings for each tweet\n",
    "# # vector_size = 200  # GloVe embedding dimension\n",
    "# tweet_vectors = loaded_df['Tweet'].swifter.apply(lambda tweet: get_weighted_avg_embedding(tweet, model=glove_model, vector_size=200, weights=tfidf_weights_all_tweets))\n",
    "# tweet_vectors = np.array(list(tweet_vectors), dtype=np.float32)\n",
    "\n",
    "# # Save the tweet vectors\n",
    "# with open(\"tweet_vectors_all_data.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(tweet_vectors, f)\n",
    "\n",
    "# print(\"Embeddings saved successfully!\")\n",
    "\n",
    "# Load the tweet vectors\n",
    "with open(\"tweet_vectors_all_data.pkl\", \"rb\") as f:\n",
    "    loaded_tweet_vectors = pickle.load(f)\n",
    "\n",
    "print(\"Embeddings loaded successfully!\")\n",
    "print(\"Loaded vectors shape:\", loaded_tweet_vectors.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Use if no period features ######\n",
    "tweet_df = pd.DataFrame(loaded_tweet_vectors)\n",
    "\n",
    "# Attach the vectors into the original dataframe\n",
    "period_features = pd.concat([loaded_df, tweet_df], axis=1)\n",
    "\n",
    "# Drop the columns that are not useful anymore\n",
    "period_features = period_features.drop(columns=['Timestamp', 'Tweet', 'HomeTeam', 'AwayTeam'])\n",
    "\n",
    "print(\"X_train_reshaped shape:\", period_features.shape)\n",
    "# Group the tweets into their corresponding periods. This way we generate an average embedding vector for each period\n",
    "period_features = period_features.groupby(['MatchID', 'PeriodID', 'ID']).mean().reset_index()\n",
    "\n",
    "# Save the tweet vectors\n",
    "with open(\"period_features.pkl\", \"wb\") as f:\n",
    "    pickle.dump(period_features, f)\n",
    "\n",
    "print(\"Period features saved successfully!\")\n",
    "\n",
    "# Load the tweet vectors\n",
    "with open(\"period_features.pkl\", \"rb\") as f:\n",
    "    loaded_period_features = pickle.load(f)\n",
    "\n",
    "print(\"Period features loaded successfully!\")\n",
    "print(\"Loaded vectors shape:\", loaded_period_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We drop the non-numerical features and keep the embeddings values for each period\n",
    "X = loaded_period_features.drop(columns=['EventType', 'MatchID', 'ID']).values\n",
    "# We extract the labels of our training samples\n",
    "y = loaded_period_features['EventType'].values\n",
    "\n",
    "\n",
    "# One-hot encode labels\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_encoded = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=SEED)\n",
    "\n",
    "# Add a time step dimension to match the LSTM input shape\n",
    "X_train_reshaped = X_train[:, None, :]  # Add a new axis for timesteps\n",
    "X_test_reshaped = X_test[:, None, :]    # Add a new axis for timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the early stopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',      # Monitor validation loss\n",
    "    patience=5,              # Stop training if no improvement after 3 epochs\n",
    "    restore_best_weights=True  # Restore the best weights when stopping\n",
    ")\n",
    "\n",
    "# Define the LSTM model with deterministic initializers\n",
    "model = Sequential([\n",
    "    tf.keras.layers.Input(shape=(1, X_train_reshaped.shape[2])),  \n",
    "    LSTM(\n",
    "        128, \n",
    "        return_sequences=False, \n",
    "        kernel_initializer=GlorotUniform(seed=SEED), \n",
    "        recurrent_initializer=Orthogonal(seed=SEED),\n",
    "        bias_initializer='zeros'\n",
    "    ),             \n",
    "    Dense(y_encoded.shape[1], activation='softmax', kernel_initializer=GlorotUniform(seed=SEED))\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_reshaped, y_train,\n",
    "                    epochs=50,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[early_stopping],  # Include the early stopping callback\n",
    "                    verbose=1)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test_reshaped, y_test, verbose=1)\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### For Kaggle submission\n",
    "\n",
    "predictions = []\n",
    "dummy_predictions = []\n",
    "# We read each file separately, we preprocess the tweets and then use the classifier to predict the labels.\n",
    "# Finally, we concatenate all predictions into a list that will eventually be concatenated and exported\n",
    "# to be submitted on Kaggle.\n",
    "for fname in sorted(os.listdir(\"eval_tweets\")):\n",
    "    val_df = pd.read_csv(\"eval_tweets/\" + fname)\n",
    "    \n",
    "    home_team, away_team = extract_teams_from_filename(fname)\n",
    "    val_df['HomeTeam'] = home_team\n",
    "    val_df['AwayTeam'] = away_team\n",
    "    \n",
    "    val_df['Tweet'] = val_df.swifter.apply(lambda row: preprocess_text(row['Tweet'], row['HomeTeam'], row['AwayTeam']), axis=1)\n",
    "\n",
    "    # Feature creation\n",
    "    # Add the length of each tweet as a feature\n",
    "    val_df['TweetLength'] = val_df['Tweet'].apply(len)\n",
    "    \n",
    "    # Add a simple tweet count feature\n",
    "    val_df['TweetCount'] = val_df.groupby(['MatchID', 'PeriodID', 'Timestamp'])['Timestamp'].transform('count')\n",
    "    \n",
    "    # Add word count as a feature\n",
    "    val_df['WordCount'] = val_df['Tweet'].apply(lambda x: len(x.split()))\n",
    "\n",
    "    tweet_vectors = val_df['Tweet'].swifter.apply(lambda tweet: get_weighted_avg_embedding(tweet, model=glove_model, vector_size=200, weights= tfidf_weights_all_tweets))\n",
    "\n",
    "    tweet_vectors = np.array(list(tweet_vectors), dtype=np.float32)\n",
    "\n",
    "    tweet_df = pd.DataFrame(tweet_vectors)\n",
    "\n",
    "    period_features_val = pd.concat([val_df, tweet_df], axis=1)\n",
    "    period_features_val = period_features_val.drop(columns=['Timestamp', 'Tweet', 'HomeTeam', 'AwayTeam'])\n",
    "    period_features_val = period_features_val.groupby(['MatchID', 'PeriodID', 'ID']).mean().reset_index()\n",
    "\n",
    "    X = period_features_val.drop(columns=['MatchID', 'ID']).values\n",
    "\n",
    "    # Reshape input for LSTM\n",
    "    X_reshaped = X[:, None, :]  # Add timestep dimension\n",
    "\n",
    "    preds = model.predict(X_reshaped)\n",
    "    preds = preds.argmax(axis=1)  # Convert probabilities to class indices\n",
    "    period_features_val['EventType'] = preds\n",
    "    predictions.append(period_features_val[['ID', 'EventType']])\n",
    "\n",
    "\n",
    "pred_df = pd.concat(predictions)\n",
    "pred_df.to_csv('LSTM_predictions.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Save TF-IDF trained on the training data\n",
    "with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "loaded_df = pd.read_pickle('preprocessed_train.pkl')\n",
    "\n",
    "# Single vectorizer for consistent vocabulary\n",
    "vectorizer = TfidfVectorizer(max_features=20000, stop_words='english', ngram_range=(1, 2))\n",
    "X = vectorizer.fit_transform(loaded_df['Tweet'])\n",
    "\n",
    "# Separate event and non-event tweets\n",
    "event_X = X[loaded_df['EventType'] == 1]\n",
    "non_event_X = X[loaded_df['EventType'] == 0]\n",
    "\n",
    "# Compute mean TF-IDF scores for sparse matrices\n",
    "event_tfidf_scores = np.array(event_X.mean(axis=0)).flatten()\n",
    "non_event_tfidf_scores = np.array(non_event_X.mean(axis=0)).flatten()\n",
    "\n",
    "# Get feature names\n",
    "tfidf_words = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Compute differences and sort\n",
    "tfidf_differences = event_tfidf_scores - non_event_tfidf_scores\n",
    "tfidf_word_differences = sorted(zip(tfidf_words, tfidf_differences), key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "# Output top words\n",
    "print(\"Top words by TF-IDF difference:\")\n",
    "print(tfidf_word_differences[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import chi2\n",
    "import pandas as pd\n",
    "\n",
    "# Convert tweets into a bag-of-words representation\n",
    "vectorizer = CountVectorizer(max_features=5000, stop_words='english', binary=True)\n",
    "X = vectorizer.fit_transform(loaded_df['Tweet'])  # Word presence (binary matrix)\n",
    "y = loaded_df['EventType']  # Binary labels: 1 (event), 0 (no event)\n",
    "\n",
    "# Perform chi-square test\n",
    "chi2_scores, p_values = chi2(X, y)\n",
    "\n",
    "# Create a DataFrame of words and chi-square scores\n",
    "chi2_results = pd.DataFrame({\n",
    "    'Word': vectorizer.get_feature_names_out(),\n",
    "    'Chi2_Score': chi2_scores,\n",
    "    'P_Value': p_values\n",
    "}).sort_values(by='Chi2_Score', ascending=False)\n",
    "\n",
    "# Display the top 10 words most associated with events\n",
    "print(\"Top event-specific words by Chi2 score:\")\n",
    "print(chi2_results.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list = [\n",
    "    'messi', 'ronaldo', 'neymar', 'beckham', 'villa', 'modric', 'suarez', 'griezmann',\n",
    "    'mbappe', 'kroos', 'kane', 'iniesta', 'xavi', 'pogba', 'drogba', 'gerard', 'hummels',\n",
    "    'ribery', 'salah', 'hazard', 'benzema', 'aguero', 'zlatan', 'kaka', 'rooney', 'degea',\n",
    "    'ozil', 'bale', 'schweinsteiger', 'pirlo', 'terry', 'alves', 'pique', 'david', 'cristiano', \n",
    "    'kramer', 'klose', 'muller', 'lahm', 'neuer', 'gotze', 'reus', 'schurrle', 'kroos', 'boateng',\n",
    "    'cromex', 'chicharito', 'torres', 'leroy', 'hernandez', 'christoph', 'vertonghen', 'john',\n",
    "    'javier', 'fernandez', 'slimani', 'matshummels', 'romero', 'porgha', 'marquez', 'guajevilla',\n",
    "    'honsui', 'cahill', 'neymarjr', 'porgha', \n",
    "]\n",
    "\n",
    "\n",
    "def remove_rows_with_names(df, name_list):\n",
    "    # Filter the DataFrame to exclude rows where the \"Word\" column matches any name in the name list\n",
    "    return df[~df['Word'].str.lower().isin(name_list)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_results = remove_rows_with_names(chi2_results, name_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chi2_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = chi2_results[:62]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for the chi² scores\n",
    "chi2_dict = dict(zip(top_words['Word'], top_words['Chi2_Score']))\n",
    "\n",
    "# Compute weighted score features for each important word\n",
    "for word in chi2_dict.keys():\n",
    "    loaded_df[f'WeightedScore_{word}'] = loaded_df['Tweet'].apply(\n",
    "        lambda tweet: tweet.split().count(word) * chi2_dict.get(word, 0)\n",
    "    )\n",
    "\n",
    "print(\"loaded_df shape:\", loaded_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_df.to_pickle('preprocessed_train_extra_weights.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = loaded_df[['TweetLength', 'WordCount']].corr()\n",
    "print(\"Correlation between TweetLength and WordCount:\")\n",
    "print(correlation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(loaded_df['TweetLength'], loaded_df['WordCount'], alpha=0.5)\n",
    "plt.xlabel('TweetLength')\n",
    "plt.ylabel('WordCount')\n",
    "plt.title('Relationship Between Tweet Length and Word Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average word length in a tweet\n",
    "loaded_df['AvgWordLength'] = loaded_df['TweetLength'] / (loaded_df['WordCount'] + 1)  # Add 1 to avoid division by zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
